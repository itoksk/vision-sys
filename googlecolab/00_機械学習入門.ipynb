{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習入門 - ディープラーニングで物体認識を体験\n",
    "\n",
    "このノートブックでは、AIの基本的な仕組みを理解し、実際に画像認識を体験します。\n",
    "\n",
    "## 学習目標\n",
    "- 機械学習とディープラーニングの基本概念を理解する\n",
    "- 事前学習済みモデル（VGG16）を使って画像認識を体験する\n",
    "- AIが画像をどのように「見ている」かを可視化する\n",
    "- YOLOとOpenCVで人物検出を体験する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install tensorflow pillow matplotlib opencv-python ultralytics numpy\n",
    "\n",
    "# 日本語フォントのインストール\n",
    "!apt-get -y install fonts-ipafont-gothic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import cv2\n",
    "from google.colab import files\n",
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "\n",
    "# 日本語フォントの設定\n",
    "plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "print(\"セットアップ完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AIの基本概念を理解しよう\n",
    "\n",
    "### 機械学習とは？\n",
    "- **従来のプログラミング**: 人間がルールを書く\n",
    "- **機械学習**: データからパターンを見つける\n",
    "\n",
    "### ディープラーニングとは？\n",
    "- 人間の脳を模倣した「ニューラルネットワーク」\n",
    "- 層を深くすることで複雑なパターンを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークの可視化\n",
    "def visualize_neural_network():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # 入力層\n",
    "    input_nodes = 4\n",
    "    for i in range(input_nodes):\n",
    "        ax.scatter(0, i - input_nodes/2, s=300, c='blue')\n",
    "        ax.text(-0.5, i - input_nodes/2, f'入力{i+1}', ha='right', va='center')\n",
    "    \n",
    "    # 隠れ層\n",
    "    hidden_nodes = 5\n",
    "    for i in range(hidden_nodes):\n",
    "        ax.scatter(2, i - hidden_nodes/2, s=300, c='green')\n",
    "        # 入力層から隠れ層への接続\n",
    "        for j in range(input_nodes):\n",
    "            ax.plot([0, 2], [j - input_nodes/2, i - hidden_nodes/2], 'gray', alpha=0.3)\n",
    "    \n",
    "    # 出力層\n",
    "    output_nodes = 2\n",
    "    for i in range(output_nodes):\n",
    "        ax.scatter(4, i - output_nodes/2, s=300, c='red')\n",
    "        ax.text(4.5, i - output_nodes/2, f'出力{i+1}', ha='left', va='center')\n",
    "        # 隠れ層から出力層への接続\n",
    "        for j in range(hidden_nodes):\n",
    "            ax.plot([2, 4], [j - hidden_nodes/2, i - output_nodes/2], 'gray', alpha=0.3)\n",
    "    \n",
    "    ax.text(0, -3, '入力層', ha='center', fontsize=12)\n",
    "    ax.text(2, -3, '隠れ層', ha='center', fontsize=12)\n",
    "    ax.text(4, -3, '出力層', ha='center', fontsize=12)\n",
    "    \n",
    "    ax.set_xlim(-1, 5)\n",
    "    ax.set_ylim(-4, 3)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('ニューラルネットワークの構造', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VGG16で画像認識を体験\n",
    "\n",
    "VGG16は、ImageNet（1000種類の物体）で学習済みの画像認識モデルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16モデルの読み込み\n",
    "print(\"VGG16モデルを読み込んでいます...\")\n",
    "model = VGG16(weights='imagenet')\n",
    "print(\"モデルの読み込み完了！\")\n",
    "print(f\"モデルの層数: {len(model.layers)}層\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像認識関数\n",
    "def predict_image(image_path):\n",
    "    \"\"\"\n",
    "    画像を読み込んで物体認識を行う\n",
    "    \"\"\"\n",
    "    # 画像の読み込みと前処理\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))  # VGG16の入力サイズ\n",
    "    \n",
    "    # numpy配列に変換\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # 予測\n",
    "    predictions = model.predict(img_array)\n",
    "    decoded = decode_predictions(predictions, top=5)[0]\n",
    "    \n",
    "    return img, decoded\n",
    "\n",
    "def display_predictions(image_path):\n",
    "    \"\"\"\n",
    "    予測結果を表示\n",
    "    \"\"\"\n",
    "    img, predictions = predict_image(image_path)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # 画像表示\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('入力画像', fontsize=14)\n",
    "    \n",
    "    # 予測結果のグラフ\n",
    "    labels = [p[1] for p in predictions]\n",
    "    scores = [p[2] for p in predictions]\n",
    "    \n",
    "    bars = ax2.barh(labels, scores)\n",
    "    ax2.set_xlabel('確率', fontsize=12)\n",
    "    ax2.set_title('認識結果 Top 5', fontsize=14)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # スコアを表示\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax2.text(score + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.1%}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n詳細な認識結果:\")\n",
    "    for i, (_, label, score) in enumerate(predictions):\n",
    "        print(f\"{i+1}. {label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像をアップロードして認識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のアップロード\n",
    "print(\"認識したい画像をアップロードしてください\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# アップロードされた画像を認識\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\n=== {filename} の認識結果 ===\")\n",
    "    display_predictions(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AIが見ている特徴を可視化\n",
    "\n",
    "CNNの中間層の出力を可視化して、AIがどのような特徴を捉えているか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(image_path, layer_name='block1_conv1'):\n",
    "    \"\"\"\n",
    "    指定した層の特徴マップを可視化\n",
    "    \"\"\"\n",
    "    # 画像の前処理\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # 特定の層の出力を取得するモデルを作成\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    feature_model = tf.keras.Model(inputs=model.input, outputs=layer_output)\n",
    "    \n",
    "    # 特徴マップを取得\n",
    "    features = feature_model.predict(img_array)\n",
    "    \n",
    "    # 可視化\n",
    "    n_features = min(features.shape[-1], 16)  # 最大16個の特徴を表示\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(features[0, :, :, i], cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'特徴 {i+1}')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name}層の特徴マップ', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 最初にアップロードされた画像で特徴を可視化\n",
    "if uploaded:\n",
    "    first_image = list(uploaded.keys())[0]\n",
    "    print(\"初期層の特徴マップ:\")\n",
    "    visualize_feature_maps(first_image, 'block1_conv1')\n",
    "    \n",
    "    print(\"\\n中間層の特徴マップ:\")\n",
    "    visualize_feature_maps(first_image, 'block3_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 人物検出の体験\n",
    "\n",
    "YOLOとOpenCVを使って人物検出を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOのセットアップ\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    print(\"YOLOv8をダウンロードしています...\")\n",
    "    yolo_model = YOLO('yolov8n.pt')  # nanoモデル（軽量）\n",
    "    USE_YOLO = True\n",
    "    print(\"YOLO準備完了！\")\n",
    "except Exception as e:\n",
    "    print(f\"YOLOの読み込みに失敗しました: {e}\")\n",
    "    print(\"OpenCVのみを使用します\")\n",
    "    USE_YOLO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_yolo(image_path):\n",
    "    \"\"\"\n",
    "    YOLOで人物検出\n",
    "    \"\"\"\n",
    "    results = yolo_model(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    person_count = 0\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # クラス0が人物\n",
    "            if box.cls == 0:\n",
    "                person_count += 1\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                cv2.putText(img, f'Person {box.conf[0]:.2f}', \n",
    "                           (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return img, person_count\n",
    "\n",
    "def detect_with_opencv(image_path):\n",
    "    \"\"\"\n",
    "    OpenCV HOGで人物検出\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    (rects, weights) = hog.detectMultiScale(img, \n",
    "                                           winStride=(4, 4),\n",
    "                                           padding=(8, 8),\n",
    "                                           scale=1.05)\n",
    "    \n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    return img, len(rects)\n",
    "\n",
    "def compare_detection_methods(image_path):\n",
    "    \"\"\"\n",
    "    検出手法の比較\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3 if USE_YOLO else 2, figsize=(15, 5))\n",
    "    \n",
    "    # 元画像\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('元画像')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # YOLO検出\n",
    "    if USE_YOLO:\n",
    "        yolo_img, yolo_count = detect_with_yolo(image_path)\n",
    "        axes[1].imshow(yolo_img)\n",
    "        axes[1].set_title(f'YOLO検出: {yolo_count}人')\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    # OpenCV検出\n",
    "    opencv_img, opencv_count = detect_with_opencv(image_path)\n",
    "    idx = 2 if USE_YOLO else 1\n",
    "    axes[idx].imshow(opencv_img)\n",
    "    axes[idx].set_title(f'OpenCV HOG検出: {opencv_count}人')\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人物が写っている画像をアップロード\n",
    "print(\"人物が写っている画像をアップロードしてください\")\n",
    "uploaded_person = files.upload()\n",
    "\n",
    "# 人物検出を実行\n",
    "for filename in uploaded_person.keys():\n",
    "    print(f\"\\n=== {filename} の人物検出結果 ===\")\n",
    "    compare_detection_methods(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 工業製品での実験\n",
    "\n",
    "工業製品の画像で認識を試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工業製品の画像をアップロード\n",
    "print(\"工業製品（ギア、ネジ、回路基板など）の画像をアップロードしてください\")\n",
    "uploaded_industrial = files.upload()\n",
    "\n",
    "# 認識を実行\n",
    "for filename in uploaded_industrial.keys():\n",
    "    print(f\"\\n=== {filename} の認識結果 ===\")\n",
    "    display_predictions(filename)\n",
    "    \n",
    "print(\"\\n💡 考察：\")\n",
    "print(\"一般的なAIモデルは工業製品の詳細な分類は苦手です。\")\n",
    "print(\"だからこそ、Teachable Machineで専用のモデルを作る必要があります！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックで学んだこと：\n",
    "1. **機械学習の基本概念** - パターンを見つける技術\n",
    "2. **画像認識の体験** - VGG16で1000種類の物体認識\n",
    "3. **特徴の可視化** - AIが注目している部分を確認\n",
    "4. **人物検出** - YOLOとOpenCVの比較\n",
    "5. **専用AIの必要性** - 工業製品には専門的なモデルが必要\n",
    "\n",
    "### 次のステップ\n",
    "- Teachable Machineで自分専用のAIモデルを作成\n",
    "- 傷検出AIの開発に挑戦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成した画像を保存\n",
    "save_dir = 'ml_intro_results'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"結果は '{save_dir}' フォルダに保存されています\")\n",
    "\n",
    "# Google Driveに保存する場合\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# !cp -r ml_intro_results /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "00_機械学習入門.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}