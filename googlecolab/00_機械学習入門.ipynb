{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ©Ÿæ¢°å­¦ç¿’å…¥é–€ - ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã§ç‰©ä½“èªè­˜ã‚’ä½“é¨“\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€AIã®åŸºæœ¬çš„ãªä»•çµ„ã¿ã‚’ç†è§£ã—ã€å®Ÿéš›ã«ç”»åƒèªè­˜ã‚’ä½“é¨“ã—ã¾ã™ã€‚\n",
    "\n",
    "## å­¦ç¿’ç›®æ¨™\n",
    "- æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µã‚’ç†è§£ã™ã‚‹\n",
    "- äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆVGG16ï¼‰ã‚’ä½¿ã£ã¦ç”»åƒèªè­˜ã‚’ä½“é¨“ã™ã‚‹\n",
    "- AIãŒç”»åƒã‚’ã©ã®ã‚ˆã†ã«ã€Œè¦‹ã¦ã„ã‚‹ã€ã‹ã‚’å¯è¦–åŒ–ã™ã‚‹\n",
    "- YOLOã¨OpenCVã§äººç‰©æ¤œå‡ºã‚’ä½“é¨“ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow pillow matplotlib opencv-python ultralytics numpy\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!apt-get -y install fonts-ipafont-gothic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import cv2\n",
    "from google.colab import files\n",
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š\n",
    "plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "print(\"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AIã®åŸºæœ¬æ¦‚å¿µã‚’ç†è§£ã—ã‚ˆã†\n",
    "\n",
    "### æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ï¼Ÿ\n",
    "- **å¾“æ¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°**: äººé–“ãŒãƒ«ãƒ¼ãƒ«ã‚’æ›¸ã\n",
    "- **æ©Ÿæ¢°å­¦ç¿’**: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "\n",
    "### ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ï¼Ÿ\n",
    "- äººé–“ã®è„³ã‚’æ¨¡å€£ã—ãŸã€Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€\n",
    "- å±¤ã‚’æ·±ãã™ã‚‹ã“ã¨ã§è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯è¦–åŒ–\n",
    "def visualize_neural_network():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # å…¥åŠ›å±¤\n",
    "    input_nodes = 4\n",
    "    for i in range(input_nodes):\n",
    "        ax.scatter(0, i - input_nodes/2, s=300, c='blue')\n",
    "        ax.text(-0.5, i - input_nodes/2, f'å…¥åŠ›{i+1}', ha='right', va='center')\n",
    "    \n",
    "    # éš ã‚Œå±¤\n",
    "    hidden_nodes = 5\n",
    "    for i in range(hidden_nodes):\n",
    "        ax.scatter(2, i - hidden_nodes/2, s=300, c='green')\n",
    "        # å…¥åŠ›å±¤ã‹ã‚‰éš ã‚Œå±¤ã¸ã®æ¥ç¶š\n",
    "        for j in range(input_nodes):\n",
    "            ax.plot([0, 2], [j - input_nodes/2, i - hidden_nodes/2], 'gray', alpha=0.3)\n",
    "    \n",
    "    # å‡ºåŠ›å±¤\n",
    "    output_nodes = 2\n",
    "    for i in range(output_nodes):\n",
    "        ax.scatter(4, i - output_nodes/2, s=300, c='red')\n",
    "        ax.text(4.5, i - output_nodes/2, f'å‡ºåŠ›{i+1}', ha='left', va='center')\n",
    "        # éš ã‚Œå±¤ã‹ã‚‰å‡ºåŠ›å±¤ã¸ã®æ¥ç¶š\n",
    "        for j in range(hidden_nodes):\n",
    "            ax.plot([2, 4], [j - hidden_nodes/2, i - output_nodes/2], 'gray', alpha=0.3)\n",
    "    \n",
    "    ax.text(0, -3, 'å…¥åŠ›å±¤', ha='center', fontsize=12)\n",
    "    ax.text(2, -3, 'éš ã‚Œå±¤', ha='center', fontsize=12)\n",
    "    ax.text(4, -3, 'å‡ºåŠ›å±¤', ha='center', fontsize=12)\n",
    "    \n",
    "    ax.set_xlim(-1, 5)\n",
    "    ax.set_ylim(-4, 3)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VGG16ã§ç”»åƒèªè­˜ã‚’ä½“é¨“\n",
    "\n",
    "VGG16ã¯ã€ImageNetï¼ˆ1000ç¨®é¡ã®ç‰©ä½“ï¼‰ã§å­¦ç¿’æ¸ˆã¿ã®ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "print(\"VGG16ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...\")\n",
    "model = VGG16(weights='imagenet')\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼\")\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ã®å±¤æ•°: {len(model.layers)}å±¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒèªè­˜é–¢æ•°\n",
    "def predict_image(image_path):\n",
    "    \"\"\"\n",
    "    ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ç‰©ä½“èªè­˜ã‚’è¡Œã†\n",
    "    \"\"\"\n",
    "    # ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))  # VGG16ã®å…¥åŠ›ã‚µã‚¤ã‚º\n",
    "    \n",
    "    # numpyé…åˆ—ã«å¤‰æ›\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    predictions = model.predict(img_array)\n",
    "    decoded = decode_predictions(predictions, top=5)[0]\n",
    "    \n",
    "    return img, decoded\n",
    "\n",
    "def display_predictions(image_path):\n",
    "    \"\"\"\n",
    "    äºˆæ¸¬çµæœã‚’è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    img, predictions = predict_image(image_path)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # ç”»åƒè¡¨ç¤º\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('å…¥åŠ›ç”»åƒ', fontsize=14)\n",
    "    \n",
    "    # äºˆæ¸¬çµæœã®ã‚°ãƒ©ãƒ•\n",
    "    labels = [p[1] for p in predictions]\n",
    "    scores = [p[2] for p in predictions]\n",
    "    \n",
    "    bars = ax2.barh(labels, scores)\n",
    "    ax2.set_xlabel('ç¢ºç‡', fontsize=12)\n",
    "    ax2.set_title('èªè­˜çµæœ Top 5', fontsize=14)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax2.text(score + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.1%}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nè©³ç´°ãªèªè­˜çµæœ:\")\n",
    "    for i, (_, label, score) in enumerate(predictions):\n",
    "        print(f\"{i+1}. {label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦èªè­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"èªè­˜ã—ãŸã„ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’èªè­˜\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\n=== {filename} ã®èªè­˜çµæœ ===\")\n",
    "    display_predictions(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AIãŒè¦‹ã¦ã„ã‚‹ç‰¹å¾´ã‚’å¯è¦–åŒ–\n",
    "\n",
    "CNNã®ä¸­é–“å±¤ã®å‡ºåŠ›ã‚’å¯è¦–åŒ–ã—ã¦ã€AIãŒã©ã®ã‚ˆã†ãªç‰¹å¾´ã‚’æ‰ãˆã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(image_path, layer_name='block1_conv1'):\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã—ãŸå±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã‚’å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    # ç”»åƒã®å‰å‡¦ç†\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # ç‰¹å®šã®å±¤ã®å‡ºåŠ›ã‚’å–å¾—ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    feature_model = tf.keras.Model(inputs=model.input, outputs=layer_output)\n",
    "    \n",
    "    # ç‰¹å¾´ãƒãƒƒãƒ—ã‚’å–å¾—\n",
    "    features = feature_model.predict(img_array)\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    n_features = min(features.shape[-1], 16)  # æœ€å¤§16å€‹ã®ç‰¹å¾´ã‚’è¡¨ç¤º\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(features[0, :, :, i], cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'ç‰¹å¾´ {i+1}')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name}å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# æœ€åˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã§ç‰¹å¾´ã‚’å¯è¦–åŒ–\n",
    "if uploaded:\n",
    "    first_image = list(uploaded.keys())[0]\n",
    "    print(\"åˆæœŸå±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—:\")\n",
    "    visualize_feature_maps(first_image, 'block1_conv1')\n",
    "    \n",
    "    print(\"\\nä¸­é–“å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—:\")\n",
    "    visualize_feature_maps(first_image, 'block3_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. äººç‰©æ¤œå‡ºã®ä½“é¨“\n",
    "\n",
    "YOLOã¨OpenCVã‚’ä½¿ã£ã¦äººç‰©æ¤œå‡ºã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    print(\"YOLOv8ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...\")\n",
    "    yolo_model = YOLO('yolov8n.pt')  # nanoãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰\n",
    "    USE_YOLO = True\n",
    "    print(\"YOLOæº–å‚™å®Œäº†ï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"YOLOã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "    print(\"OpenCVã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "    USE_YOLO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_yolo(image_path):\n",
    "    \"\"\"\n",
    "    YOLOã§äººç‰©æ¤œå‡º\n",
    "    \"\"\"\n",
    "    results = yolo_model(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    person_count = 0\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # ã‚¯ãƒ©ã‚¹0ãŒäººç‰©\n",
    "            if box.cls == 0:\n",
    "                person_count += 1\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                cv2.putText(img, f'Person {box.conf[0]:.2f}', \n",
    "                           (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return img, person_count\n",
    "\n",
    "def detect_with_opencv(image_path):\n",
    "    \"\"\"\n",
    "    OpenCV HOGã§äººç‰©æ¤œå‡º\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    (rects, weights) = hog.detectMultiScale(img, \n",
    "                                           winStride=(4, 4),\n",
    "                                           padding=(8, 8),\n",
    "                                           scale=1.05)\n",
    "    \n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    return img, len(rects)\n",
    "\n",
    "def compare_detection_methods(image_path):\n",
    "    \"\"\"\n",
    "    æ¤œå‡ºæ‰‹æ³•ã®æ¯”è¼ƒ\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3 if USE_YOLO else 2, figsize=(15, 5))\n",
    "    \n",
    "    # å…ƒç”»åƒ\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('å…ƒç”»åƒ')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # YOLOæ¤œå‡º\n",
    "    if USE_YOLO:\n",
    "        yolo_img, yolo_count = detect_with_yolo(image_path)\n",
    "        axes[1].imshow(yolo_img)\n",
    "        axes[1].set_title(f'YOLOæ¤œå‡º: {yolo_count}äºº')\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    # OpenCVæ¤œå‡º\n",
    "    opencv_img, opencv_count = detect_with_opencv(image_path)\n",
    "    idx = 2 if USE_YOLO else 1\n",
    "    axes[idx].imshow(opencv_img)\n",
    "    axes[idx].set_title(f'OpenCV HOGæ¤œå‡º: {opencv_count}äºº')\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äººç‰©ãŒå†™ã£ã¦ã„ã‚‹ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"äººç‰©ãŒå†™ã£ã¦ã„ã‚‹ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "uploaded_person = files.upload()\n",
    "\n",
    "# äººç‰©æ¤œå‡ºã‚’å®Ÿè¡Œ\n",
    "for filename in uploaded_person.keys():\n",
    "    print(f\"\\n=== {filename} ã®äººç‰©æ¤œå‡ºçµæœ ===\")\n",
    "    compare_detection_methods(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å·¥æ¥­è£½å“ã§ã®å®Ÿé¨“\n",
    "\n",
    "å·¥æ¥­è£½å“ã®ç”»åƒã§èªè­˜ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å·¥æ¥­è£½å“ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"å·¥æ¥­è£½å“ï¼ˆã‚®ã‚¢ã€ãƒã‚¸ã€å›è·¯åŸºæ¿ãªã©ï¼‰ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "uploaded_industrial = files.upload()\n",
    "\n",
    "# èªè­˜ã‚’å®Ÿè¡Œ\n",
    "for filename in uploaded_industrial.keys():\n",
    "    print(f\"\\n=== {filename} ã®èªè­˜çµæœ ===\")\n",
    "    display_predictions(filename)\n",
    "    \n",
    "print(\"\\nğŸ’¡ è€ƒå¯Ÿï¼š\")\n",
    "print(\"ä¸€èˆ¬çš„ãªAIãƒ¢ãƒ‡ãƒ«ã¯å·¥æ¥­è£½å“ã®è©³ç´°ãªåˆ†é¡ã¯è‹¦æ‰‹ã§ã™ã€‚\")\n",
    "print(\"ã ã‹ã‚‰ã“ãã€Teachable Machineã§å°‚ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã‚“ã ã“ã¨ï¼š\n",
    "1. **æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ** - ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹æŠ€è¡“\n",
    "2. **ç”»åƒèªè­˜ã®ä½“é¨“** - VGG16ã§1000ç¨®é¡ã®ç‰©ä½“èªè­˜\n",
    "3. **ç‰¹å¾´ã®å¯è¦–åŒ–** - AIãŒæ³¨ç›®ã—ã¦ã„ã‚‹éƒ¨åˆ†ã‚’ç¢ºèª\n",
    "4. **äººç‰©æ¤œå‡º** - YOLOã¨OpenCVã®æ¯”è¼ƒ\n",
    "5. **å°‚ç”¨AIã®å¿…è¦æ€§** - å·¥æ¥­è£½å“ã«ã¯å°‚é–€çš„ãªãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "- Teachable Machineã§è‡ªåˆ†å°‚ç”¨ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
    "- å‚·æ¤œå‡ºAIã®é–‹ç™ºã«æŒ‘æˆ¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½œæˆã—ãŸç”»åƒã‚’ä¿å­˜\n",
    "save_dir = 'ml_intro_results'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"çµæœã¯ '{save_dir}' ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "\n",
    "# Google Driveã«ä¿å­˜ã™ã‚‹å ´åˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# !cp -r ml_intro_results /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "00_æ©Ÿæ¢°å­¦ç¿’å…¥é–€.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}