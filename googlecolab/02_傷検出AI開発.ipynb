{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# å‚·æ¤œå‡ºAIé–‹ç™º - å®Œå…¨ç‰ˆã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€è£½é€ ç¾å ´ã§ä½¿ãˆã‚‹å‚·æ¤œå‡ºAIã‚¢ãƒ—ãƒªã‚’é–‹ç™ºã—ã¾ã™ã€‚\n\n## é‡è¦ï¼šTeachable Machineã®ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ–¹æ³•\n1. Teachable Machineã§ã€Œãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€ã‚’ã‚¯ãƒªãƒƒã‚¯\n2. ã€ŒTensorflowã€ã‚¿ãƒ–ã‚’é¸æŠ\n3. ã€ŒKerasã€ã‚’é¸æŠ\n4. ã€Œãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ã‚’ã‚¯ãƒªãƒƒã‚¯\nâ†’ `keras_model.h5`ã¨`labels.txt`ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™\n\n## é–‹ç™ºã®æµã‚Œ\n1. ãƒ‡ãƒ¼ã‚¿åé›†ã¨æ•´ç†\n2. Teachable Machineã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ\n3. Google Vision APIã¨ã®é€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n4. Gradioã§å®Ÿç”¨çš„ãªWebã‚¢ãƒ—ãƒªä½œæˆ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow gradio opencv-python pillow numpy matplotlib\n",
    "!pip install google-cloud-vision  # Vision APIç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ\n",
    "import os\n",
    "project_path = '/content/drive/MyDrive/damage_detection_project'\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "os.makedirs(f'{project_path}/dataset/good', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/dataset/bad', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/models', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/results', exist_ok=True)\n",
    "\n",
    "print(\"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š\n",
    "!apt-get -y install fonts-ipafont-gothic\n",
    "plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self, project_path):\n",
    "        self.project_path = project_path\n",
    "        self.metadata = {\n",
    "            'good': [],\n",
    "            'bad': [],\n",
    "            'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    \n",
    "    def upload_images(self, category='good'):\n",
    "        \"\"\"\n",
    "        ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ \n",
    "        category: 'good' (è‰¯å“) or 'bad' (ä¸è‰¯å“)\n",
    "        \"\"\"\n",
    "        print(f\"{'è‰¯å“' if category == 'good' else 'ä¸è‰¯å“'}ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çµ±ä¸€å½¢å¼ã«å¤‰æ›´\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            new_filename = f\"{category}_{timestamp}_{filename}\"\n",
    "            \n",
    "            # ç”»åƒã‚’ä¿å­˜\n",
    "            img = Image.open(filename)\n",
    "            save_path = f\"{self.project_path}/dataset/{category}/{new_filename}\"\n",
    "            img.save(save_path)\n",
    "            \n",
    "            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²\n",
    "            self.metadata[category].append({\n",
    "                'filename': new_filename,\n",
    "                'original_filename': filename,\n",
    "                'timestamp': timestamp,\n",
    "                'size': img.size\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ“ {filename} ã‚’ {category} ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    def show_dataset_summary(self):\n",
    "        \"\"\"\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ã‚’è¡¨ç¤º\n",
    "        \"\"\"\n",
    "        good_count = len(os.listdir(f\"{self.project_path}/dataset/good\"))\n",
    "        bad_count = len(os.listdir(f\"{self.project_path}/dataset/bad\"))\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        categories = ['è‰¯å“', 'ä¸è‰¯å“']\n",
    "        counts = [good_count, bad_count]\n",
    "        colors = ['green', 'red']\n",
    "        \n",
    "        bars = ax.bar(categories, counts, color=colors)\n",
    "        ax.set_ylabel('ç”»åƒæ•°', fontsize=12)\n",
    "        ax.set_title('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦', fontsize=14)\n",
    "        \n",
    "        # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   str(count), ha='center', va='bottom')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nåˆè¨ˆ: {good_count + bad_count}æš\")\n",
    "        print(f\"è‰¯å“: {good_count}æš\")\n",
    "        print(f\"ä¸è‰¯å“: {bad_count}æš\")\n",
    "        \n",
    "        if good_count < 50 or bad_count < 50:\n",
    "            print(\"\\nâš ï¸ æ³¨æ„: å„ã‚«ãƒ†ã‚´ãƒª50æšä»¥ä¸Šã®ç”»åƒã‚’æ¨å¥¨ã—ã¾ã™\")\n",
    "    \n",
    "    def save_metadata(self):\n",
    "        \"\"\"\n",
    "        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\n",
    "        \"\"\"\n",
    "        with open(f\"{self.project_path}/dataset/metadata.json\", 'w') as f:\n",
    "            json.dump(self.metadata, f, ensure_ascii=False, indent=2)\n",
    "        print(\"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åé›†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ\n",
    "collector = DataCollector(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‰¯å“ç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "collector.upload_images('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸è‰¯å“ç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "collector.upload_images('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦ã‚’è¡¨ç¤º\n",
    "collector.show_dataset_summary()\n",
    "collector.save_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Teachable Machineãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ”¹è‰¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teachable Machineã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"Teachable Machineã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸ keras_model.h5 ã¨ labels.txt ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•\n",
    "import shutil\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, f\"{project_path}/models/{filename}\")\n",
    "    print(f\"âœ“ {filename} ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DamageDetector:\n",
    "    def __init__(self, model_path, labels_path):\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ©ãƒ™ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        self.model = tf.keras.models.load_model(model_path, compile=False)\n",
    "        \n",
    "        with open(labels_path, 'r', encoding='utf-8') as f:\n",
    "            self.labels = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²\n",
    "        self.performance_log = []\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        ç”»åƒã®å‰å‡¦ç†\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ\n",
    "            img = Image.open(image).convert('RGB')\n",
    "        else:\n",
    "            # numpyé…åˆ—ã®å ´åˆ\n",
    "            img = Image.fromarray(image).convert('RGB')\n",
    "        \n",
    "        # ãƒªã‚µã‚¤ã‚ºã¨æ­£è¦åŒ–\n",
    "        img = img.resize((224, 224))\n",
    "        img_array = np.array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        return img_array, img\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        ç”»åƒã®äºˆæ¸¬\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # å‰å‡¦ç†\n",
    "        img_array, original_img = self.preprocess_image(image)\n",
    "        \n",
    "        # äºˆæ¸¬\n",
    "        predictions = self.model.predict(img_array, verbose=0)\n",
    "        \n",
    "        # å‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²\n",
    "        process_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # çµæœã‚’æ•´ç†\n",
    "        results = {}\n",
    "        for i, label in enumerate(self.labels):\n",
    "            clean_label = label.split(' ', 1)[1] if ' ' in label else label\n",
    "            results[clean_label] = float(predictions[0][i])\n",
    "        \n",
    "        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°ã«è¿½åŠ \n",
    "        self.performance_log.append({\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'process_time': process_time,\n",
    "            'result': max(results, key=results.get)\n",
    "        })\n",
    "        \n",
    "        return results, original_img, process_time\n",
    "    \n",
    "    def analyze_with_confidence(self, image):\n",
    "        \"\"\"\n",
    "        ä¿¡é ¼åº¦ã‚’å«ã‚€è©³ç´°ãªåˆ†æ\n",
    "        \"\"\"\n",
    "        results, img, process_time = self.predict(image)\n",
    "        \n",
    "        # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯\n",
    "        bad_prob = results.get('ä¸è‰¯å“', results.get('bad', 0))\n",
    "        good_prob = results.get('è‰¯å“', results.get('good', 0))\n",
    "        \n",
    "        # åˆ¤å®šåŸºæº–\n",
    "        if bad_prob > 0.8:\n",
    "            status = \"ä¸è‰¯å“\"\n",
    "            confidence = \"é«˜\"\n",
    "            action = \"å»ƒæ£„ã¾ãŸã¯å†åŠ å·¥ã‚’æ¨å¥¨\"\n",
    "            color = \"red\"\n",
    "        elif bad_prob > 0.5:\n",
    "            status = \"è¦ç¢ºèª\"\n",
    "            confidence = \"ä¸­\"\n",
    "            action = \"ç›®è¦–ã§ã®å†ç¢ºèªã‚’æ¨å¥¨\"\n",
    "            color = \"orange\"\n",
    "        elif bad_prob > 0.3:\n",
    "            status = \"è¦æ³¨æ„\"\n",
    "            confidence = \"ä½\"\n",
    "            action = \"å“è³ªç®¡ç†è€…ã«ã‚ˆã‚‹ç¢ºèªã‚’æ¨å¥¨\"\n",
    "            color = \"yellow\"\n",
    "        else:\n",
    "            status = \"è‰¯å“\"\n",
    "            confidence = \"é«˜\"\n",
    "            action = \"æ¬¡å·¥ç¨‹ã¸é€²ã‚ã¦å•é¡Œã‚ã‚Šã¾ã›ã‚“\"\n",
    "            color = \"green\"\n",
    "        \n",
    "        analysis = {\n",
    "            'status': status,\n",
    "            'confidence': confidence,\n",
    "            'action': action,\n",
    "            'color': color,\n",
    "            'probabilities': results,\n",
    "            'process_time': process_time,\n",
    "            'image': img\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def visualize_analysis(self, analysis):\n",
    "        \"\"\"\n",
    "        åˆ†æçµæœã®å¯è¦–åŒ–\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # ç”»åƒè¡¨ç¤º\n",
    "        ax1.imshow(analysis['image'])\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(f\"åˆ¤å®š: {analysis['status']}\", fontsize=16, \n",
    "                     color=analysis['color'], weight='bold')\n",
    "        \n",
    "        # ç¢ºç‡ã®å¯è¦–åŒ–\n",
    "        labels = list(analysis['probabilities'].keys())\n",
    "        values = list(analysis['probabilities'].values())\n",
    "        colors = ['red' if 'ä¸è‰¯' in l or 'bad' in l else 'green' for l in labels]\n",
    "        \n",
    "        bars = ax2.barh(labels, values, color=colors)\n",
    "        ax2.set_xlabel('ç¢ºç‡', fontsize=12)\n",
    "        ax2.set_title('è©³ç´°åˆ†æçµæœ', fontsize=14)\n",
    "        ax2.set_xlim(0, 1)\n",
    "        \n",
    "        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax2.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{value:.1%}', va='center')\n",
    "        \n",
    "        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º\n",
    "        plt.figtext(0.5, 0.02, f\"æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {analysis['action']}\", \n",
    "                   ha='center', fontsize=12, style='italic')\n",
    "        plt.figtext(0.5, -0.02, f\"å‡¦ç†æ™‚é–“: {analysis['process_time']:.3f}ç§’\", \n",
    "                   ha='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# æ¤œå‡ºå™¨ã®åˆæœŸåŒ–\n",
    "detector = DamageDetector(\n",
    "    model_path=f\"{project_path}/models/keras_model.h5\",\n",
    "    labels_path=f\"{project_path}/models/labels.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Google Vision APIé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "ã‚ˆã‚Šé«˜åº¦ãªåˆ†æã®ãŸã‚ã«Google Vision APIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision API ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ï¼‰\n",
    "USE_VISION_API = False  # Vision APIã‚’ä½¿ã†å ´åˆã¯Trueã«å¤‰æ›´\n",
    "\n",
    "if USE_VISION_API:\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    \n",
    "    class VisionAPIAnalyzer:\n",
    "        def __init__(self):\n",
    "            self.client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        def analyze_image(self, image_path):\n",
    "            \"\"\"\n",
    "            Vision APIã§ç”»åƒã‚’åˆ†æ\n",
    "            \"\"\"\n",
    "            with io.open(image_path, 'rb') as image_file:\n",
    "                content = image_file.read()\n",
    "            \n",
    "            image = vision.Image(content=content)\n",
    "            \n",
    "            # ç‰©ä½“æ¤œå‡º\n",
    "            objects = self.client.object_localization(image=image).localized_object_annotations\n",
    "            \n",
    "            # ãƒ©ãƒ™ãƒ«æ¤œå‡º\n",
    "            labels = self.client.label_detection(image=image).label_annotations\n",
    "            \n",
    "            # ç”»åƒã®ç‰¹æ€§\n",
    "            properties = self.client.image_properties(image=image).image_properties_annotation\n",
    "            \n",
    "            return {\n",
    "                'objects': [(obj.name, obj.score) for obj in objects],\n",
    "                'labels': [(label.description, label.score) for label in labels],\n",
    "                'dominant_colors': properties.dominant_colors\n",
    "            }\n",
    "    \n",
    "    vision_analyzer = VisionAPIAnalyzer()\n",
    "else:\n",
    "    print(\"Vision APIã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚Teachable Machineã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradioã‚’ä½¿ã£ãŸå®Ÿç”¨çš„ãªWebã‚¢ãƒ—ãƒª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# æ¤œæŸ»å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "inspection_history = []\n",
    "\n",
    "def inspect_part(image):\n",
    "    \"\"\"\n",
    "    éƒ¨å“æ¤œæŸ»ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None, \"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\", None, None\n",
    "    \n",
    "    # åˆ†æå®Ÿè¡Œ\n",
    "    analysis = detector.analyze_with_confidence(image)\n",
    "    \n",
    "    # çµæœã®æ•´ç†\n",
    "    probabilities = analysis['probabilities']\n",
    "    \n",
    "    # åˆ¤å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä½œæˆ\n",
    "    status_emoji = {\n",
    "        \"è‰¯å“\": \"âœ…\",\n",
    "        \"ä¸è‰¯å“\": \"âŒ\",\n",
    "        \"è¦ç¢ºèª\": \"âš ï¸\",\n",
    "        \"è¦æ³¨æ„\": \"ğŸ”\"\n",
    "    }\n",
    "    \n",
    "    message = f\"\"\"\n",
    "{status_emoji.get(analysis['status'], '')} **åˆ¤å®šçµæœ: {analysis['status']}**\n",
    "\n",
    "**ä¿¡é ¼åº¦**: {analysis['confidence']}\n",
    "**æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: {analysis['action']}\n",
    "**å‡¦ç†æ™‚é–“**: {analysis['process_time']:.3f}ç§’\n",
    "\"\"\"\n",
    "    \n",
    "    # å±¥æ­´ã«è¿½åŠ \n",
    "    inspection_history.append({\n",
    "        'æ¤œæŸ»æ—¥æ™‚': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'åˆ¤å®š': analysis['status'],\n",
    "        'ä¿¡é ¼åº¦': analysis['confidence'],\n",
    "        'ä¸è‰¯å“ç¢ºç‡': f\"{probabilities.get('ä¸è‰¯å“', 0):.1%}\",\n",
    "        'å‡¦ç†æ™‚é–“': f\"{analysis['process_time']:.3f}ç§’\"\n",
    "    })\n",
    "    \n",
    "    # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆæœ€æ–°10ä»¶ï¼‰\n",
    "    history_df = pd.DataFrame(inspection_history[-10:])\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±ã®ä½œæˆ\n",
    "    if len(inspection_history) > 0:\n",
    "        total = len(inspection_history)\n",
    "        good_count = sum(1 for h in inspection_history if h['åˆ¤å®š'] == 'è‰¯å“')\n",
    "        bad_count = sum(1 for h in inspection_history if h['åˆ¤å®š'] == 'ä¸è‰¯å“')\n",
    "        \n",
    "        stats_message = f\"\"\"\n",
    "### ğŸ“Š æœ¬æ—¥ã®æ¤œæŸ»çµ±è¨ˆ\n",
    "- ç·æ¤œæŸ»æ•°: {total}ä»¶\n",
    "- è‰¯å“: {good_count}ä»¶ ({good_count/total*100:.1f}%)\n",
    "- ä¸è‰¯å“: {bad_count}ä»¶ ({bad_count/total*100:.1f}%)\n",
    "- å¹³å‡å‡¦ç†æ™‚é–“: {np.mean([float(h['å‡¦ç†æ™‚é–“'].replace('ç§’', '')) for h in inspection_history]):.3f}ç§’\n",
    "\"\"\"\n",
    "    else:\n",
    "        stats_message = \"çµ±è¨ˆæƒ…å ±ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“\"\n",
    "    \n",
    "    return probabilities, message, history_df, stats_message\n",
    "\n",
    "def export_history():\n",
    "    \"\"\"\n",
    "    æ¤œæŸ»å±¥æ­´ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n",
    "    \"\"\"\n",
    "    if len(inspection_history) > 0:\n",
    "        df = pd.DataFrame(inspection_history)\n",
    "        filename = f\"inspection_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        return f\"å±¥æ­´ã‚’ {filename} ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ\"\n",
    "    return \"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“\"\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ CSS\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    font-family: 'Noto Sans JP', sans-serif;\n",
    "}\n",
    ".output-markdown h3 {\n",
    "    color: #1f77b4;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ\n",
    "with gr.Blocks(title=\"å‚·æ¤œå‡ºAIæ¤œæŸ»ã‚·ã‚¹ãƒ†ãƒ \", theme=gr.themes.Soft(), css=custom_css) as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ğŸ­ è£½é€ éƒ¨å“ å‚·æ¤œå‡ºAIã‚·ã‚¹ãƒ†ãƒ \n",
    "    \n",
    "    é‡‘å±éƒ¨å“ã®è¡¨é¢æ¤œæŸ»ã‚’AIãŒã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Œæ¤œæŸ»é–‹å§‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            input_image = gr.Image(\n",
    "                label=\"æ¤œæŸ»å¯¾è±¡ã®ç”»åƒ\",\n",
    "                type=\"numpy\",\n",
    "                height=300\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                inspect_btn = gr.Button(\"ğŸ” æ¤œæŸ»é–‹å§‹\", variant=\"primary\", size=\"lg\")\n",
    "                clear_btn = gr.Button(\"ğŸ—‘ï¸ ã‚¯ãƒªã‚¢\", variant=\"secondary\")\n",
    "            \n",
    "            # ä½¿ã„æ–¹ã®ã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³\n",
    "            with gr.Accordion(\"ğŸ“– ä½¿ã„æ–¹\", open=False):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### æ’®å½±ã®ãƒã‚¤ãƒ³ãƒˆ\n",
    "                1. **ç…§æ˜**: å‡ä¸€ãªç…§æ˜ä¸‹ã§æ’®å½±\n",
    "                2. **è§’åº¦**: éƒ¨å“ã«å¯¾ã—ã¦æ­£é¢ã‹ã‚‰æ’®å½±\n",
    "                3. **è·é›¢**: éƒ¨å“å…¨ä½“ãŒç”»è§’ã«åã¾ã‚‹ã‚ˆã†ã«\n",
    "                4. **èƒŒæ™¯**: ã‚·ãƒ³ãƒ—ãƒ«ãªèƒŒæ™¯ã‚’ä½¿ç”¨\n",
    "                \n",
    "                ### åˆ¤å®šåŸºæº–\n",
    "                - **è‰¯å“**: ä¸è‰¯å“ç¢ºç‡ 30%æœªæº€\n",
    "                - **è¦æ³¨æ„**: ä¸è‰¯å“ç¢ºç‡ 30-50%\n",
    "                - **è¦ç¢ºèª**: ä¸è‰¯å“ç¢ºç‡ 50-80%\n",
    "                - **ä¸è‰¯å“**: ä¸è‰¯å“ç¢ºç‡ 80%ä»¥ä¸Š\n",
    "                \"\"\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            output_label = gr.Label(\n",
    "                label=\"åˆ¤å®šç¢ºç‡\",\n",
    "                num_top_classes=2\n",
    "            )\n",
    "            \n",
    "            output_message = gr.Markdown(\n",
    "                label=\"åˆ¤å®šçµæœ\",\n",
    "                value=\"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¤œæŸ»ã‚’é–‹å§‹ã—ã¦ãã ã•ã„\"\n",
    "            )\n",
    "            \n",
    "            stats_display = gr.Markdown(\n",
    "                label=\"çµ±è¨ˆæƒ…å ±\"\n",
    "            )\n",
    "    \n",
    "    # æ¤œæŸ»å±¥æ­´ã®ã‚¿ãƒ–\n",
    "    with gr.Tab(\"æ¤œæŸ»å±¥æ­´\"):\n",
    "        history_table = gr.Dataframe(\n",
    "            label=\"æœ€è¿‘ã®æ¤œæŸ»çµæœï¼ˆæœ€æ–°10ä»¶ï¼‰\",\n",
    "            headers=[\"æ¤œæŸ»æ—¥æ™‚\", \"åˆ¤å®š\", \"ä¿¡é ¼åº¦\", \"ä¸è‰¯å“ç¢ºç‡\", \"å‡¦ç†æ™‚é–“\"],\n",
    "            datatype=[\"str\", \"str\", \"str\", \"str\", \"str\"],\n",
    "            row_count=10\n",
    "        )\n",
    "        \n",
    "        export_btn = gr.Button(\"ğŸ“¥ å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\", variant=\"secondary\")\n",
    "        export_message = gr.Textbox(label=\"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆçŠ¶æ…‹\", interactive=False)\n",
    "    \n",
    "    # ãƒãƒƒãƒå‡¦ç†ã‚¿ãƒ–\n",
    "    with gr.Tab(\"ãƒãƒƒãƒå‡¦ç†\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### è¤‡æ•°ç”»åƒã®ä¸€æ‹¬æ¤œæŸ»\n",
    "        è¤‡æ•°ã®ç”»åƒã‚’ä¸€åº¦ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¤œæŸ»ã§ãã¾ã™ã€‚\n",
    "        \"\"\")\n",
    "        \n",
    "        batch_input = gr.File(\n",
    "            label=\"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰\",\n",
    "            file_count=\"multiple\",\n",
    "            file_types=[\"image\"]\n",
    "        )\n",
    "        \n",
    "        batch_btn = gr.Button(\"ğŸš€ ä¸€æ‹¬æ¤œæŸ»é–‹å§‹\", variant=\"primary\")\n",
    "        batch_output = gr.Dataframe(\n",
    "            label=\"ãƒãƒƒãƒå‡¦ç†çµæœ\",\n",
    "            headers=[\"ãƒ•ã‚¡ã‚¤ãƒ«å\", \"åˆ¤å®š\", \"ä¸è‰¯å“ç¢ºç‡\", \"å‡¦ç†æ™‚é–“\"]\n",
    "        )\n",
    "    \n",
    "    # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼\n",
    "    inspect_btn.click(\n",
    "        fn=inspect_part,\n",
    "        inputs=input_image,\n",
    "        outputs=[output_label, output_message, history_table, stats_display]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=lambda: (None, None, \"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¤œæŸ»ã‚’é–‹å§‹ã—ã¦ãã ã•ã„\", None, None),\n",
    "        outputs=[input_image, output_label, output_message, history_table, stats_display]\n",
    "    )\n",
    "    \n",
    "    export_btn.click(\n",
    "        fn=export_history,\n",
    "        outputs=export_message\n",
    "    )\n",
    "    \n",
    "    # ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…\n",
    "    def batch_process(files):\n",
    "        if not files:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        results = []\n",
    "        for file in files:\n",
    "            analysis = detector.analyze_with_confidence(file.name)\n",
    "            results.append({\n",
    "                'ãƒ•ã‚¡ã‚¤ãƒ«å': os.path.basename(file.name),\n",
    "                'åˆ¤å®š': analysis['status'],\n",
    "                'ä¸è‰¯å“ç¢ºç‡': f\"{analysis['probabilities'].get('ä¸è‰¯å“', 0):.1%}\",\n",
    "                'å‡¦ç†æ™‚é–“': f\"{analysis['process_time']:.3f}ç§’\"\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    batch_btn.click(\n",
    "        fn=batch_process,\n",
    "        inputs=batch_input,\n",
    "        outputs=batch_output\n",
    "    )\n",
    "\n",
    "# ã‚¢ãƒ—ãƒªã‚’èµ·å‹•\n",
    "app.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance():\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    test_results = []\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡\n",
    "    for category in ['good', 'bad']:\n",
    "        folder_path = f\"{project_path}/dataset/{category}\"\n",
    "        images = os.listdir(folder_path)[:10]  # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰10æšãšã¤\n",
    "        \n",
    "        for img_file in images:\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            analysis = detector.analyze_with_confidence(img_path)\n",
    "            \n",
    "            # æ­£è§£ãƒ©ãƒ™ãƒ«ã¨äºˆæ¸¬ã‚’æ¯”è¼ƒ\n",
    "            true_label = 'è‰¯å“' if category == 'good' else 'ä¸è‰¯å“'\n",
    "            predicted_label = analysis['status']\n",
    "            \n",
    "            test_results.append({\n",
    "                'true': true_label,\n",
    "                'predicted': predicted_label,\n",
    "                'confidence': analysis['probabilities'].get('ä¸è‰¯å“', 0),\n",
    "                'correct': true_label == predicted_label or \n",
    "                          (true_label == 'ä¸è‰¯å“' and predicted_label in ['ä¸è‰¯å“', 'è¦ç¢ºèª'])\n",
    "            })\n",
    "    \n",
    "    # ç²¾åº¦è¨ˆç®—\n",
    "    accuracy = sum(r['correct'] for r in test_results) / len(test_results)\n",
    "    \n",
    "    # æ··åŒè¡Œåˆ—ã®ä½œæˆ\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    true_labels = [r['true'] for r in test_results]\n",
    "    pred_labels = [r['predicted'] for r in test_results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # æ··åŒè¡Œåˆ—\n",
    "    plt.subplot(2, 2, 1)\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=['è‰¯å“', 'ä¸è‰¯å“', 'è¦ç¢ºèª', 'è¦æ³¨æ„'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('æ··åŒè¡Œåˆ—')\n",
    "    plt.ylabel('æ­£è§£ãƒ©ãƒ™ãƒ«')\n",
    "    plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')\n",
    "    \n",
    "    # ç²¾åº¦ã‚°ãƒ©ãƒ•\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(['ç²¾åº¦'], [accuracy], color='green')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f'ãƒ¢ãƒ‡ãƒ«ç²¾åº¦: {accuracy:.1%}')\n",
    "    \n",
    "    # å‡¦ç†æ™‚é–“ã®åˆ†å¸ƒ\n",
    "    plt.subplot(2, 2, 3)\n",
    "    process_times = [float(log['process_time']) for log in detector.performance_log]\n",
    "    if process_times:\n",
    "        plt.hist(process_times, bins=20, color='blue', alpha=0.7)\n",
    "        plt.xlabel('å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰')\n",
    "        plt.ylabel('é »åº¦')\n",
    "        plt.title('å‡¦ç†æ™‚é–“ã®åˆ†å¸ƒ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{project_path}/results/performance_report.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ\n",
    "    report = f\"\"\"\n",
    "# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼\n",
    "- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_results)}ä»¶\n",
    "- å…¨ä½“ç²¾åº¦: {accuracy:.1%}\n",
    "- å¹³å‡å‡¦ç†æ™‚é–“: {np.mean(process_times):.3f}ç§’\n",
    "\n",
    "## æ¨å¥¨äº‹é …\n",
    "1. ç¾åœ¨ã®ç²¾åº¦ã¯{accuracy:.1%}ã§ã™ã€‚{'è‰¯å¥½ãªæ€§èƒ½ã§ã™ã€‚' if accuracy > 0.8 else 'ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦å†å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚'}\n",
    "2. å‡¦ç†æ™‚é–“ã¯å¹³å‡{np.mean(process_times):.3f}ç§’ã§ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œæŸ»ã«é©ã—ã¦ã„ã¾ã™ã€‚\n",
    "3. èª¤æ¤œå‡ºã‚’æ¸›ã‚‰ã™ãŸã‚ã€ç…§æ˜æ¡ä»¶ã‚’çµ±ä¸€ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n",
    "\n",
    "## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "- ã‚ˆã‚Šå¤šæ§˜ãªä¸è‰¯å“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¼ã‚¿åé›†\n",
    "- ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®å‹•ä½œæ¤œè¨¼\n",
    "- ç¾å ´ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{project_path}/results/evaluation_report.md\", 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(report)\n",
    "    return accuracy, report\n",
    "\n",
    "# è©•ä¾¡å®Ÿè¡Œ\n",
    "if len(os.listdir(f\"{project_path}/dataset/good\")) > 0:\n",
    "    accuracy, report = evaluate_model_performance()\n",
    "else:\n",
    "    print(\"è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚ã¨ä»Šå¾Œã®å±•é–‹\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å®Ÿè£…ã—ãŸå†…å®¹ï¼š\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿åé›†ã¨ç®¡ç†**\n",
    "   - ä½“ç³»çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰\n",
    "   - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²\n",
    "\n",
    "2. **AIãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…**\n",
    "   - Teachable Machineãƒ¢ãƒ‡ãƒ«ã®çµ±åˆ\n",
    "   - è©³ç´°ãªåˆ†ææ©Ÿèƒ½\n",
    "\n",
    "3. **å®Ÿç”¨çš„ãªWebã‚¢ãƒ—ãƒª**\n",
    "   - ç›´æ„Ÿçš„ãªUI\n",
    "   - ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½\n",
    "   - æ¤œæŸ»å±¥æ­´ã®ç®¡ç†\n",
    "\n",
    "4. **æ€§èƒ½è©•ä¾¡**\n",
    "   - ç²¾åº¦æ¸¬å®š\n",
    "   - ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "\n",
    "### ä»Šå¾Œã®æ”¹è‰¯æ¡ˆ\n",
    "- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚«ãƒ¡ãƒ©é€£æº\n",
    "- ä¸è‰¯ç®‡æ‰€ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º\n",
    "- è¤‡æ•°ã®ä¸è‰¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡\n",
    "- ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¸ã®å±•é–‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}