{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 傷検出AI開発 - シンプル版（Teachable Machine不要）\n",
    "\n",
    "このノートブックでは、TensorFlow/Kerasを直接使って傷検出AIを作成します。\n",
    "より理解しやすく、カスタマイズも簡単な実装です。\n",
    "\n",
    "## なぜこちらの方法も学ぶ？\n",
    "- AIの仕組みをより深く理解できる\n",
    "- 自由にモデルをカスタマイズできる\n",
    "- 実際の開発現場に近い体験ができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install tensorflow opencv-python pillow numpy matplotlib gradio scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# 日本語フォントの設定\n",
    "!apt-get -y install fonts-ipafont-gothic\n",
    "plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "print(\"✅ セットアップ完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Driveの準備とデータ収集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveをマウント\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# プロジェクトフォルダの作成\n",
    "project_path = '/content/drive/MyDrive/damage_detection_simple'\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "os.makedirs(f'{project_path}/dataset/good', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/dataset/bad', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/models', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/results', exist_ok=True)\n",
    "\n",
    "print(\"📁 フォルダ構造:\")\n",
    "print(f\"{project_path}/\")\n",
    "print(\"├── dataset/\")\n",
    "print(\"│   ├── good/  # 良品画像\")\n",
    "print(\"│   └── bad/   # 不良品画像\")\n",
    "print(\"├── models/    # 保存されたモデル\")\n",
    "print(\"└── results/   # 結果とレポート\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データアップロード用の関数\n",
    "def upload_images(category='good'):\n",
    "    \"\"\"\n",
    "    画像をアップロードしてデータセットフォルダに保存\n",
    "    \"\"\"\n",
    "    print(f\"📤 {'良品' if category == 'good' else '不良品'}の画像をアップロードしてください\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    count = 0\n",
    "    for filename in uploaded.keys():\n",
    "        # 保存先パス\n",
    "        save_path = f\"{project_path}/dataset/{category}/{filename}\"\n",
    "        \n",
    "        # 画像を開いて保存（形式を統一）\n",
    "        img = Image.open(filename)\n",
    "        img = img.convert('RGB')  # RGBに変換\n",
    "        img.save(save_path, 'JPEG')\n",
    "        \n",
    "        count += 1\n",
    "        print(f\"✓ {filename} を保存しました\")\n",
    "    \n",
    "    print(f\"\\n✅ {count}枚の画像を {category} フォルダに保存しました\")\n",
    "    return count\n",
    "\n",
    "# 使用例（コメントを外して実行）\n",
    "# upload_images('good')  # 良品画像をアップロード\n",
    "# upload_images('bad')   # 不良品画像をアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの確認\n",
    "def check_dataset():\n",
    "    \"\"\"\n",
    "    データセットの状態を確認\n",
    "    \"\"\"\n",
    "    good_images = os.listdir(f\"{project_path}/dataset/good\")\n",
    "    bad_images = os.listdir(f\"{project_path}/dataset/bad\")\n",
    "    \n",
    "    good_count = len([f for f in good_images if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    bad_count = len([f for f in bad_images if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    # グラフで表示\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    categories = ['良品', '不良品']\n",
    "    counts = [good_count, bad_count]\n",
    "    colors = ['green', 'red']\n",
    "    \n",
    "    bars = plt.bar(categories, counts, color=colors)\n",
    "    plt.title('データセットの内訳', fontsize=16)\n",
    "    plt.ylabel('画像数', fontsize=12)\n",
    "    \n",
    "    # 数値を表示\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                str(count), ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📊 データセット統計\")\n",
    "    print(f\"良品: {good_count}枚\")\n",
    "    print(f\"不良品: {bad_count}枚\")\n",
    "    print(f\"合計: {good_count + bad_count}枚\")\n",
    "    \n",
    "    if good_count < 20 or bad_count < 20:\n",
    "        print(\"\\n⚠️ 警告: 各カテゴリ20枚以上の画像を推奨します\")\n",
    "    \n",
    "    return good_count, bad_count\n",
    "\n",
    "# データセットを確認\n",
    "check_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. シンプルなCNNモデルの構築\n",
    "\n",
    "畳み込みニューラルネットワーク（CNN）を使って、画像から特徴を学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの前処理とデータセット作成\n",
    "def create_dataset(data_dir, image_size=(128, 128), batch_size=16):\n",
    "    \"\"\"\n",
    "    画像データセットを作成\n",
    "    \"\"\"\n",
    "    # データ拡張の設定（学習データの多様性を増やす）\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),  # 左右反転\n",
    "        layers.RandomRotation(0.1),        # 回転\n",
    "        layers.RandomZoom(0.1),            # ズーム\n",
    "    ])\n",
    "    \n",
    "    # データセットを作成\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,  # 20%を検証用に\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'  # 2クラス分類\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    # クラス名を取得\n",
    "    class_names = dataset.class_names\n",
    "    print(f\"クラス: {class_names}\")\n",
    "    \n",
    "    # データ拡張を適用\n",
    "    dataset = dataset.map(lambda x, y: (data_augmentation(x), y))\n",
    "    \n",
    "    # パフォーマンス最適化\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset, val_dataset, class_names\n",
    "\n",
    "# データセットの準備\n",
    "train_ds, val_ds, class_names = create_dataset(f\"{project_path}/dataset\")\n",
    "print(f\"\\n✅ データセット準備完了\")\n",
    "print(f\"クラス: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル画像の表示\n",
    "def show_sample_images(dataset, class_names):\n",
    "    \"\"\"\n",
    "    データセットからサンプル画像を表示\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 1バッチ取得\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(min(9, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            \n",
    "            # ラベルを表示（0: bad, 1: good）\n",
    "            label_idx = int(labels[i])\n",
    "            label_name = class_names[label_idx]\n",
    "            color = 'green' if label_name == 'good' else 'red'\n",
    "            \n",
    "            plt.title(f'{label_name}', color=color)\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle('データセットのサンプル画像', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# サンプル表示\n",
    "show_sample_images(train_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルなCNNモデルの構築\n",
    "def create_simple_cnn(input_shape=(128, 128, 3)):\n",
    "    \"\"\"\n",
    "    シンプルで理解しやすいCNNモデル\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # 入力の正規化\n",
    "        layers.Rescaling(1./255, input_shape=input_shape),\n",
    "        \n",
    "        # 畳み込み層1（特徴を抽出）\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # 畳み込み層2（より複雑な特徴を抽出）\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # 畳み込み層3（さらに複雑な特徴を抽出）\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # ドロップアウト（過学習防止）\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # 全結合層\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # 2クラス分類\n",
    "    ])\n",
    "    \n",
    "    # モデルのコンパイル\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# モデルの作成\n",
    "model = create_simple_cnn()\n",
    "\n",
    "# モデルの構造を表示\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の実行\n",
    "def train_model(model, train_ds, val_ds, epochs=20):\n",
    "    \"\"\"\n",
    "    モデルの学習を実行\n",
    "    \"\"\"\n",
    "    # コールバックの設定\n",
    "    callbacks = [\n",
    "        # 改善が見られない場合は学習率を下げる\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # 改善が見られない場合は早期終了\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"🚀 学習を開始します...\")\n",
    "    \n",
    "    # 学習実行\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ 学習完了！\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 学習実行（エポック数は調整可能）\n",
    "history = train_model(model, train_ds, val_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果の可視化\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    学習履歴をグラフで表示\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # 精度のグラフ\n",
    "    ax1.plot(history.history['accuracy'], label='学習データ')\n",
    "    ax1.plot(history.history['val_accuracy'], label='検証データ')\n",
    "    ax1.set_title('モデルの精度', fontsize=14)\n",
    "    ax1.set_xlabel('エポック')\n",
    "    ax1.set_ylabel('精度')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 損失のグラフ\n",
    "    ax2.plot(history.history['loss'], label='学習データ')\n",
    "    ax2.plot(history.history['val_loss'], label='検証データ')\n",
    "    ax2.set_title('モデルの損失', fontsize=14)\n",
    "    ax2.set_xlabel('エポック')\n",
    "    ax2.set_ylabel('損失')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 最終的な精度を表示\n",
    "    final_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"\\n📊 最終的な検証精度: {final_accuracy:.1%}\")\n",
    "    \n",
    "    if final_accuracy < 0.8:\n",
    "        print(\"💡 ヒント: データを増やすか、学習エポック数を増やしてみてください\")\n",
    "    else:\n",
    "        print(\"🎉 良い精度が出ています！\")\n",
    "\n",
    "# 学習結果を表示\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデルの保存と読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "def save_model(model, model_name=\"damage_detector\"):\n",
    "    \"\"\"\n",
    "    モデルを保存\n",
    "    \"\"\"\n",
    "    # 保存パス\n",
    "    save_path = f\"{project_path}/models/{model_name}.h5\"\n",
    "    \n",
    "    # モデルを保存\n",
    "    model.save(save_path)\n",
    "    print(f\"💾 モデルを保存しました: {save_path}\")\n",
    "    \n",
    "    # クラス名も保存\n",
    "    with open(f\"{project_path}/models/class_names.txt\", 'w') as f:\n",
    "        for name in class_names:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# モデルを保存\n",
    "model_path = save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 予測機能の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測クラスの実装\n",
    "class SimpleDamageDetector:\n",
    "    \"\"\"\n",
    "    シンプルな傷検出器\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, image_size=(128, 128)):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.image_size = image_size\n",
    "        self.history = []\n",
    "        \n",
    "        # クラス名の読み込み\n",
    "        class_names_path = model_path.replace('.h5', '').replace('damage_detector', 'class_names.txt')\n",
    "        try:\n",
    "            with open(class_names_path, 'r') as f:\n",
    "                self.class_names = [line.strip() for line in f.readlines()]\n",
    "        except:\n",
    "            self.class_names = ['bad', 'good']  # デフォルト\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        画像の前処理\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # ファイルパスの場合\n",
    "            img = Image.open(image)\n",
    "        else:\n",
    "            # numpy配列の場合\n",
    "            img = Image.fromarray(image)\n",
    "        \n",
    "        # RGBに変換してリサイズ\n",
    "        img = img.convert('RGB')\n",
    "        img = img.resize(self.image_size)\n",
    "        \n",
    "        # numpy配列に変換\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        return img_array, img\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        画像から傷を検出\n",
    "        \"\"\"\n",
    "        # 前処理\n",
    "        img_array, original_img = self.preprocess_image(image)\n",
    "        \n",
    "        # 予測\n",
    "        prediction = self.model.predict(img_array, verbose=0)[0][0]\n",
    "        \n",
    "        # 結果を整理（0に近い=bad, 1に近い=good）\n",
    "        if prediction < 0.5:\n",
    "            result = 'bad'\n",
    "            confidence = 1 - prediction\n",
    "        else:\n",
    "            result = 'good'\n",
    "            confidence = prediction\n",
    "        \n",
    "        # 履歴に追加\n",
    "        self.history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'result': result,\n",
    "            'confidence': float(confidence)\n",
    "        })\n",
    "        \n",
    "        return result, confidence, original_img\n",
    "    \n",
    "    def analyze(self, image):\n",
    "        \"\"\"\n",
    "        詳細な分析結果を返す\n",
    "        \"\"\"\n",
    "        result, confidence, img = self.predict(image)\n",
    "        \n",
    "        # 判定基準\n",
    "        if result == 'bad' and confidence > 0.8:\n",
    "            status = \"不良品\"\n",
    "            action = \"廃棄または再加工を推奨\"\n",
    "            emoji = \"❌\"\n",
    "        elif result == 'bad' and confidence > 0.6:\n",
    "            status = \"要確認\"\n",
    "            action = \"目視での再確認を推奨\"\n",
    "            emoji = \"⚠️\"\n",
    "        else:\n",
    "            status = \"良品\"\n",
    "            action = \"次工程へ進めて問題ありません\"\n",
    "            emoji = \"✅\"\n",
    "        \n",
    "        return {\n",
    "            'status': status,\n",
    "            'confidence': confidence,\n",
    "            'action': action,\n",
    "            'emoji': emoji,\n",
    "            'image': img\n",
    "        }\n",
    "\n",
    "# 検出器を初期化\n",
    "detector = SimpleDamageDetector(model_path)\n",
    "print(\"✅ 検出器の準備完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト実行\n",
    "print(\"🖼️ テスト画像をアップロードしてください\")\n",
    "test_files = files.upload()\n",
    "\n",
    "for filename in test_files.keys():\n",
    "    print(f\"\\n=== {filename} の分析 ===\")\n",
    "    \n",
    "    # 分析実行\n",
    "    analysis = detector.analyze(filename)\n",
    "    \n",
    "    # 結果表示\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(analysis['image'])\n",
    "    plt.title(f\"{analysis['emoji']} {analysis['status']} (信頼度: {analysis['confidence']:.1%})\", \n",
    "             fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"判定: {analysis['status']}\")\n",
    "    print(f\"信頼度: {analysis['confidence']:.1%}\")\n",
    "    print(f\"推奨アクション: {analysis['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradioを使ったWebアプリ化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプル版Webアプリ\n",
    "def create_simple_app(detector):\n",
    "    \"\"\"\n",
    "    シンプルで使いやすいWebアプリ\n",
    "    \"\"\"\n",
    "    def process_image(image):\n",
    "        if image is None:\n",
    "            return \"画像をアップロードしてください\"\n",
    "        \n",
    "        # 分析実行\n",
    "        analysis = detector.analyze(image)\n",
    "        \n",
    "        # 結果メッセージ\n",
    "        message = f\"\"\"\n",
    "{analysis['emoji']} **判定結果: {analysis['status']}**\n",
    "\n",
    "**信頼度**: {analysis['confidence']:.1%}\n",
    "**推奨アクション**: {analysis['action']}\n",
    "\"\"\"\n",
    "        return message\n",
    "    \n",
    "    # インターフェース作成\n",
    "    app = gr.Interface(\n",
    "        fn=process_image,\n",
    "        inputs=gr.Image(label=\"検査画像\", type=\"numpy\"),\n",
    "        outputs=gr.Markdown(label=\"判定結果\"),\n",
    "        title=\"🔍 傷検出AIシステム（シンプル版）\",\n",
    "        description=\"部品の画像をアップロードすると、AIが傷の有無を判定します。\",\n",
    "        examples=[],  # サンプル画像があれば追加\n",
    "        theme=gr.themes.Soft()\n",
    "    )\n",
    "    \n",
    "    return app\n",
    "\n",
    "# アプリ作成と起動\n",
    "app = create_simple_app(detector)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. モデルの評価と改善"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の作成\n",
    "def evaluate_model(model, test_dataset):\n",
    "    \"\"\"\n",
    "    モデルの詳細な評価\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 予測と正解を収集\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        for i in range(len(labels)):\n",
    "            y_true.append(int(labels[i]))\n",
    "            y_pred.append(1 if predictions[i] > 0.5 else 0)\n",
    "    \n",
    "    # 混同行列\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['不良品', '良品'],\n",
    "                yticklabels=['不良品', '良品'])\n",
    "    plt.title('混同行列', fontsize=16)\n",
    "    plt.ylabel('実際のラベル')\n",
    "    plt.xlabel('予測ラベル')\n",
    "    plt.show()\n",
    "    \n",
    "    # 詳細レポート\n",
    "    print(\"\\n📊 分類レポート:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                              target_names=['不良品', '良品']))\n",
    "    \n",
    "    # 精度計算\n",
    "    accuracy = sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)\n",
    "    print(f\"\\n✅ 全体精度: {accuracy:.1%}\")\n",
    "    \n",
    "    return cm, accuracy\n",
    "\n",
    "# 評価実行（検証データセットで）\n",
    "# cm, acc = evaluate_model(model, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このシンプル版では：\n",
    "\n",
    "1. **直接的な実装**: TensorFlow/Kerasを使った分かりやすいCNN\n",
    "2. **カスタマイズ可能**: モデルの構造を自由に変更できる\n",
    "3. **学習過程が見える**: 精度の向上を確認しながら学習\n",
    "4. **軽量**: Teachable Machineより小さいモデルサイズ\n",
    "\n",
    "### 改善のヒント\n",
    "\n",
    "1. **データを増やす**: 各クラス100枚以上が理想\n",
    "2. **モデルを調整**: 層を増やしたり、ニューロン数を変更\n",
    "3. **学習率の調整**: optimizerのlearning_rateを変更\n",
    "4. **転移学習**: 事前学習済みモデル（MobileNet等）を使用\n",
    "\n",
    "この実装により、AIの仕組みをより深く理解できます！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
