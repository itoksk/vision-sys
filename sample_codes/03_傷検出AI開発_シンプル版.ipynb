{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å‚·æ¤œå‡ºAIé–‹ç™º - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆTeachable Machineä¸è¦ï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€TensorFlow/Kerasã‚’ç›´æ¥ä½¿ã£ã¦å‚·æ¤œå‡ºAIã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "ã‚ˆã‚Šç†è§£ã—ã‚„ã™ãã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚‚ç°¡å˜ãªå®Ÿè£…ã§ã™ã€‚\n",
    "\n",
    "## ãªãœã“ã¡ã‚‰ã®æ–¹æ³•ã‚‚å­¦ã¶ï¼Ÿ\n",
    "- AIã®ä»•çµ„ã¿ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã§ãã‚‹\n",
    "- è‡ªç”±ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã‚‹\n",
    "- å®Ÿéš›ã®é–‹ç™ºç¾å ´ã«è¿‘ã„ä½“é¨“ãŒã§ãã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow opencv-python pillow numpy matplotlib gradio scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š\n",
    "!apt-get -y install fonts-ipafont-gothic\n",
    "plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "print(\"âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Driveã®æº–å‚™ã¨ãƒ‡ãƒ¼ã‚¿åé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ\n",
    "project_path = '/content/drive/MyDrive/damage_detection_simple'\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "os.makedirs(f'{project_path}/dataset/good', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/dataset/bad', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/models', exist_ok=True)\n",
    "os.makedirs(f'{project_path}/results', exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ :\")\n",
    "print(f\"{project_path}/\")\n",
    "print(\"â”œâ”€â”€ dataset/\")\n",
    "print(\"â”‚   â”œâ”€â”€ good/  # è‰¯å“ç”»åƒ\")\n",
    "print(\"â”‚   â””â”€â”€ bad/   # ä¸è‰¯å“ç”»åƒ\")\n",
    "print(\"â”œâ”€â”€ models/    # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«\")\n",
    "print(\"â””â”€â”€ results/   # çµæœã¨ãƒ¬ãƒãƒ¼ãƒˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®é–¢æ•°\n",
    "def upload_images(category='good'):\n",
    "    \"\"\"\n",
    "    ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¤ {'è‰¯å“' if category == 'good' else 'ä¸è‰¯å“'}ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    count = 0\n",
    "    for filename in uploaded.keys():\n",
    "        # ä¿å­˜å…ˆãƒ‘ã‚¹\n",
    "        save_path = f\"{project_path}/dataset/{category}/{filename}\"\n",
    "        \n",
    "        # ç”»åƒã‚’é–‹ã„ã¦ä¿å­˜ï¼ˆå½¢å¼ã‚’çµ±ä¸€ï¼‰\n",
    "        img = Image.open(filename)\n",
    "        img = img.convert('RGB')  # RGBã«å¤‰æ›\n",
    "        img.save(save_path, 'JPEG')\n",
    "        \n",
    "        count += 1\n",
    "        print(f\"âœ“ {filename} ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    print(f\"\\nâœ… {count}æšã®ç”»åƒã‚’ {category} ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "    return count\n",
    "\n",
    "# ä½¿ç”¨ä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œï¼‰\n",
    "# upload_images('good')  # è‰¯å“ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "# upload_images('bad')   # ä¸è‰¯å“ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª\n",
    "def check_dataset():\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "    \"\"\"\n",
    "    good_images = os.listdir(f\"{project_path}/dataset/good\")\n",
    "    bad_images = os.listdir(f\"{project_path}/dataset/bad\")\n",
    "    \n",
    "    good_count = len([f for f in good_images if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    bad_count = len([f for f in bad_images if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    # ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    categories = ['è‰¯å“', 'ä¸è‰¯å“']\n",
    "    counts = [good_count, bad_count]\n",
    "    colors = ['green', 'red']\n",
    "    \n",
    "    bars = plt.bar(categories, counts, color=colors)\n",
    "    plt.title('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…è¨³', fontsize=16)\n",
    "    plt.ylabel('ç”»åƒæ•°', fontsize=12)\n",
    "    \n",
    "    # æ•°å€¤ã‚’è¡¨ç¤º\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                str(count), ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ\")\n",
    "    print(f\"è‰¯å“: {good_count}æš\")\n",
    "    print(f\"ä¸è‰¯å“: {bad_count}æš\")\n",
    "    print(f\"åˆè¨ˆ: {good_count + bad_count}æš\")\n",
    "    \n",
    "    if good_count < 20 or bad_count < 20:\n",
    "        print(\"\\nâš ï¸ è­¦å‘Š: å„ã‚«ãƒ†ã‚´ãƒª20æšä»¥ä¸Šã®ç”»åƒã‚’æ¨å¥¨ã—ã¾ã™\")\n",
    "    \n",
    "    return good_count, bad_count\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª\n",
    "check_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ã‚·ãƒ³ãƒ—ãƒ«ãªCNNãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
    "\n",
    "ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰ã‚’ä½¿ã£ã¦ã€ç”»åƒã‹ã‚‰ç‰¹å¾´ã‚’å­¦ç¿’ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "def create_dataset(data_dir, image_size=(128, 128), batch_size=16):\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "    \"\"\"\n",
    "    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®è¨­å®šï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ã‚’å¢—ã‚„ã™ï¼‰\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),  # å·¦å³åè»¢\n",
    "        layers.RandomRotation(0.1),        # å›è»¢\n",
    "        layers.RandomZoom(0.1),            # ã‚ºãƒ¼ãƒ \n",
    "    ])\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,  # 20%ã‚’æ¤œè¨¼ç”¨ã«\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'  # 2ã‚¯ãƒ©ã‚¹åˆ†é¡\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='binary'\n",
    "    )\n",
    "    \n",
    "    # ã‚¯ãƒ©ã‚¹åã‚’å–å¾—\n",
    "    class_names = dataset.class_names\n",
    "    print(f\"ã‚¯ãƒ©ã‚¹: {class_names}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨\n",
    "    dataset = dataset.map(lambda x, y: (data_augmentation(x), y))\n",
    "    \n",
    "    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset, val_dataset, class_names\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "train_ds, val_ds, class_names = create_dataset(f\"{project_path}/dataset\")\n",
    "print(f\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®è¡¨ç¤º\n",
    "def show_sample_images(dataset, class_names):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 1ãƒãƒƒãƒå–å¾—\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(min(9, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            \n",
    "            # ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºï¼ˆ0: bad, 1: goodï¼‰\n",
    "            label_idx = int(labels[i])\n",
    "            label_name = class_names[label_idx]\n",
    "            color = 'green' if label_name == 'good' else 'red'\n",
    "            \n",
    "            plt.title(f'{label_name}', color=color)\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "show_sample_images(train_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚·ãƒ³ãƒ—ãƒ«ãªCNNãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
    "def create_simple_cnn(input_shape=(128, 128, 3)):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„CNNãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # å…¥åŠ›ã®æ­£è¦åŒ–\n",
    "        layers.Rescaling(1./255, input_shape=input_shape),\n",
    "        \n",
    "        # ç•³ã¿è¾¼ã¿å±¤1ï¼ˆç‰¹å¾´ã‚’æŠ½å‡ºï¼‰\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # ç•³ã¿è¾¼ã¿å±¤2ï¼ˆã‚ˆã‚Šè¤‡é›‘ãªç‰¹å¾´ã‚’æŠ½å‡ºï¼‰\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # ç•³ã¿è¾¼ã¿å±¤3ï¼ˆã•ã‚‰ã«è¤‡é›‘ãªç‰¹å¾´ã‚’æŠ½å‡ºï¼‰\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # å…¨çµåˆå±¤\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # 2ã‚¯ãƒ©ã‚¹åˆ†é¡\n",
    "    ])\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = create_simple_cnn()\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’è¡¨ç¤º\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "def train_model(model, train_ds, val_ds, epochs=20):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œ\n",
    "    \"\"\"\n",
    "    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š\n",
    "    callbacks = [\n",
    "        # æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã„å ´åˆã¯å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã„å ´åˆã¯æ—©æœŸçµ‚äº†\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    \n",
    "    # å­¦ç¿’å®Ÿè¡Œ\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… å­¦ç¿’å®Œäº†ï¼\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# å­¦ç¿’å®Ÿè¡Œï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ã¯èª¿æ•´å¯èƒ½ï¼‰\n",
    "history = train_model(model, train_ds, val_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’çµæœã®å¯è¦–åŒ–\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’å±¥æ­´ã‚’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # ç²¾åº¦ã®ã‚°ãƒ©ãƒ•\n",
    "    ax1.plot(history.history['accuracy'], label='å­¦ç¿’ãƒ‡ãƒ¼ã‚¿')\n",
    "    ax1.plot(history.history['val_accuracy'], label='æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿')\n",
    "    ax1.set_title('ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦', fontsize=14)\n",
    "    ax1.set_xlabel('ã‚¨ãƒãƒƒã‚¯')\n",
    "    ax1.set_ylabel('ç²¾åº¦')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # æå¤±ã®ã‚°ãƒ©ãƒ•\n",
    "    ax2.plot(history.history['loss'], label='å­¦ç¿’ãƒ‡ãƒ¼ã‚¿')\n",
    "    ax2.plot(history.history['val_loss'], label='æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿')\n",
    "    ax2.set_title('ãƒ¢ãƒ‡ãƒ«ã®æå¤±', fontsize=14)\n",
    "    ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')\n",
    "    ax2.set_ylabel('æå¤±')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æœ€çµ‚çš„ãªç²¾åº¦ã‚’è¡¨ç¤º\n",
    "    final_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nğŸ“Š æœ€çµ‚çš„ãªæ¤œè¨¼ç²¾åº¦: {final_accuracy:.1%}\")\n",
    "    \n",
    "    if final_accuracy < 0.8:\n",
    "        print(\"ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã‹ã€å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã—ã¦ã¿ã¦ãã ã•ã„\")\n",
    "    else:\n",
    "        print(\"ğŸ‰ è‰¯ã„ç²¾åº¦ãŒå‡ºã¦ã„ã¾ã™ï¼\")\n",
    "\n",
    "# å­¦ç¿’çµæœã‚’è¡¨ç¤º\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "def save_model(model, model_name=\"damage_detector\"):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    \"\"\"\n",
    "    # ä¿å­˜ãƒ‘ã‚¹\n",
    "    save_path = f\"{project_path}/models/{model_name}.h5\"\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    model.save(save_path)\n",
    "    print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}\")\n",
    "    \n",
    "    # ã‚¯ãƒ©ã‚¹åã‚‚ä¿å­˜\n",
    "    with open(f\"{project_path}/models/class_names.txt\", 'w') as f:\n",
    "        for name in class_names:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "model_path = save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. äºˆæ¸¬æ©Ÿèƒ½ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…\n",
    "class SimpleDamageDetector:\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ãƒ—ãƒ«ãªå‚·æ¤œå‡ºå™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, image_size=(128, 128)):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.image_size = image_size\n",
    "        self.history = []\n",
    "        \n",
    "        # ã‚¯ãƒ©ã‚¹åã®èª­ã¿è¾¼ã¿\n",
    "        class_names_path = model_path.replace('.h5', '').replace('damage_detector', 'class_names.txt')\n",
    "        try:\n",
    "            with open(class_names_path, 'r') as f:\n",
    "                self.class_names = [line.strip() for line in f.readlines()]\n",
    "        except:\n",
    "            self.class_names = ['bad', 'good']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        ç”»åƒã®å‰å‡¦ç†\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ\n",
    "            img = Image.open(image)\n",
    "        else:\n",
    "            # numpyé…åˆ—ã®å ´åˆ\n",
    "            img = Image.fromarray(image)\n",
    "        \n",
    "        # RGBã«å¤‰æ›ã—ã¦ãƒªã‚µã‚¤ã‚º\n",
    "        img = img.convert('RGB')\n",
    "        img = img.resize(self.image_size)\n",
    "        \n",
    "        # numpyé…åˆ—ã«å¤‰æ›\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        return img_array, img\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        ç”»åƒã‹ã‚‰å‚·ã‚’æ¤œå‡º\n",
    "        \"\"\"\n",
    "        # å‰å‡¦ç†\n",
    "        img_array, original_img = self.preprocess_image(image)\n",
    "        \n",
    "        # äºˆæ¸¬\n",
    "        prediction = self.model.predict(img_array, verbose=0)[0][0]\n",
    "        \n",
    "        # çµæœã‚’æ•´ç†ï¼ˆ0ã«è¿‘ã„=bad, 1ã«è¿‘ã„=goodï¼‰\n",
    "        if prediction < 0.5:\n",
    "            result = 'bad'\n",
    "            confidence = 1 - prediction\n",
    "        else:\n",
    "            result = 'good'\n",
    "            confidence = prediction\n",
    "        \n",
    "        # å±¥æ­´ã«è¿½åŠ \n",
    "        self.history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'result': result,\n",
    "            'confidence': float(confidence)\n",
    "        })\n",
    "        \n",
    "        return result, confidence, original_img\n",
    "    \n",
    "    def analyze(self, image):\n",
    "        \"\"\"\n",
    "        è©³ç´°ãªåˆ†æçµæœã‚’è¿”ã™\n",
    "        \"\"\"\n",
    "        result, confidence, img = self.predict(image)\n",
    "        \n",
    "        # åˆ¤å®šåŸºæº–\n",
    "        if result == 'bad' and confidence > 0.8:\n",
    "            status = \"ä¸è‰¯å“\"\n",
    "            action = \"å»ƒæ£„ã¾ãŸã¯å†åŠ å·¥ã‚’æ¨å¥¨\"\n",
    "            emoji = \"âŒ\"\n",
    "        elif result == 'bad' and confidence > 0.6:\n",
    "            status = \"è¦ç¢ºèª\"\n",
    "            action = \"ç›®è¦–ã§ã®å†ç¢ºèªã‚’æ¨å¥¨\"\n",
    "            emoji = \"âš ï¸\"\n",
    "        else:\n",
    "            status = \"è‰¯å“\"\n",
    "            action = \"æ¬¡å·¥ç¨‹ã¸é€²ã‚ã¦å•é¡Œã‚ã‚Šã¾ã›ã‚“\"\n",
    "            emoji = \"âœ…\"\n",
    "        \n",
    "        return {\n",
    "            'status': status,\n",
    "            'confidence': confidence,\n",
    "            'action': action,\n",
    "            'emoji': emoji,\n",
    "            'image': img\n",
    "        }\n",
    "\n",
    "# æ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–\n",
    "detector = SimpleDamageDetector(model_path)\n",
    "print(\"âœ… æ¤œå‡ºå™¨ã®æº–å‚™å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "print(\"ğŸ–¼ï¸ ãƒ†ã‚¹ãƒˆç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "test_files = files.upload()\n",
    "\n",
    "for filename in test_files.keys():\n",
    "    print(f\"\\n=== {filename} ã®åˆ†æ ===\")\n",
    "    \n",
    "    # åˆ†æå®Ÿè¡Œ\n",
    "    analysis = detector.analyze(filename)\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(analysis['image'])\n",
    "    plt.title(f\"{analysis['emoji']} {analysis['status']} (ä¿¡é ¼åº¦: {analysis['confidence']:.1%})\", \n",
    "             fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"åˆ¤å®š: {analysis['status']}\")\n",
    "    print(f\"ä¿¡é ¼åº¦: {analysis['confidence']:.1%}\")\n",
    "    print(f\"æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {analysis['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradioã‚’ä½¿ã£ãŸWebã‚¢ãƒ—ãƒªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆWebã‚¢ãƒ—ãƒª\n",
    "def create_simple_app(detector):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ãƒ—ãƒ«ã§ä½¿ã„ã‚„ã™ã„Webã‚¢ãƒ—ãƒª\n",
    "    \"\"\"\n",
    "    def process_image(image):\n",
    "        if image is None:\n",
    "            return \"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\"\n",
    "        \n",
    "        # åˆ†æå®Ÿè¡Œ\n",
    "        analysis = detector.analyze(image)\n",
    "        \n",
    "        # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "        message = f\"\"\"\n",
    "{analysis['emoji']} **åˆ¤å®šçµæœ: {analysis['status']}**\n",
    "\n",
    "**ä¿¡é ¼åº¦**: {analysis['confidence']:.1%}\n",
    "**æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: {analysis['action']}\n",
    "\"\"\"\n",
    "        return message\n",
    "    \n",
    "    # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ\n",
    "    app = gr.Interface(\n",
    "        fn=process_image,\n",
    "        inputs=gr.Image(label=\"æ¤œæŸ»ç”»åƒ\", type=\"numpy\"),\n",
    "        outputs=gr.Markdown(label=\"åˆ¤å®šçµæœ\"),\n",
    "        title=\"ğŸ” å‚·æ¤œå‡ºAIã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰\",\n",
    "        description=\"éƒ¨å“ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€AIãŒå‚·ã®æœ‰ç„¡ã‚’åˆ¤å®šã—ã¾ã™ã€‚\",\n",
    "        examples=[],  # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒãŒã‚ã‚Œã°è¿½åŠ \n",
    "        theme=gr.themes.Soft()\n",
    "    )\n",
    "    \n",
    "    return app\n",
    "\n",
    "# ã‚¢ãƒ—ãƒªä½œæˆã¨èµ·å‹•\n",
    "app = create_simple_app(detector)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æ”¹å–„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··åŒè¡Œåˆ—ã®ä½œæˆ\n",
    "def evaluate_model(model, test_dataset):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ãªè©•ä¾¡\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # äºˆæ¸¬ã¨æ­£è§£ã‚’åé›†\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        for i in range(len(labels)):\n",
    "            y_true.append(int(labels[i]))\n",
    "            y_pred.append(1 if predictions[i] > 0.5 else 0)\n",
    "    \n",
    "    # æ··åŒè¡Œåˆ—\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['ä¸è‰¯å“', 'è‰¯å“'],\n",
    "                yticklabels=['ä¸è‰¯å“', 'è‰¯å“'])\n",
    "    plt.title('æ··åŒè¡Œåˆ—', fontsize=16)\n",
    "    plt.ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')\n",
    "    plt.xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')\n",
    "    plt.show()\n",
    "    \n",
    "    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "    print(\"\\nğŸ“Š åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                              target_names=['ä¸è‰¯å“', 'è‰¯å“']))\n",
    "    \n",
    "    # ç²¾åº¦è¨ˆç®—\n",
    "    accuracy = sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)\n",
    "    print(f\"\\nâœ… å…¨ä½“ç²¾åº¦: {accuracy:.1%}\")\n",
    "    \n",
    "    return cm, accuracy\n",
    "\n",
    "# è©•ä¾¡å®Ÿè¡Œï¼ˆæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ï¼‰\n",
    "# cm, acc = evaluate_model(model, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆã§ã¯ï¼š\n",
    "\n",
    "1. **ç›´æ¥çš„ãªå®Ÿè£…**: TensorFlow/Kerasã‚’ä½¿ã£ãŸåˆ†ã‹ã‚Šã‚„ã™ã„CNN\n",
    "2. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½**: ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’è‡ªç”±ã«å¤‰æ›´ã§ãã‚‹\n",
    "3. **å­¦ç¿’éç¨‹ãŒè¦‹ãˆã‚‹**: ç²¾åº¦ã®å‘ä¸Šã‚’ç¢ºèªã—ãªãŒã‚‰å­¦ç¿’\n",
    "4. **è»½é‡**: Teachable Machineã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º\n",
    "\n",
    "### æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™**: å„ã‚¯ãƒ©ã‚¹100æšä»¥ä¸ŠãŒç†æƒ³\n",
    "2. **ãƒ¢ãƒ‡ãƒ«ã‚’èª¿æ•´**: å±¤ã‚’å¢—ã‚„ã—ãŸã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’å¤‰æ›´\n",
    "3. **å­¦ç¿’ç‡ã®èª¿æ•´**: optimizerã®learning_rateã‚’å¤‰æ›´\n",
    "4. **è»¢ç§»å­¦ç¿’**: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆMobileNetç­‰ï¼‰ã‚’ä½¿ç”¨\n",
    "\n",
    "ã“ã®å®Ÿè£…ã«ã‚ˆã‚Šã€AIã®ä»•çµ„ã¿ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã§ãã¾ã™ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
