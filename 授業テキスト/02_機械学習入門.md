# 第2時：ディープラーニングで物体認識を体験しよう

## 🎯 今日の目標
AIがどうやって画像を認識するのか理解して、実際に体験する！

---

## 📝 機械学習とディープラーニング

### 人間の学習 vs 機械の学習

**人間の場合**
1. 猫を見る → 「これは猫だ」と教わる
2. 何度も見る → 特徴を覚える（耳の形、ヒゲ、鳴き声）
3. 新しい猫を見る → 「これも猫だ！」と分かる

**機械の場合**
1. たくさんの猫の画像を見せる（データ）
2. 特徴を自動で見つける（学習）
3. 新しい画像を判定する（予測）

### ディープラーニングの仕組み

```
入力（画像） → 特徴抽出 → パターン認識 → 結果（猫/犬）
```

画像は「ピクセル（点）の集まり」として認識されます：
- 各ピクセルには色の情報（RGB）
- AIはピクセルのパターンから特徴を学習

---

## 🚀 Google Colabを使ってみよう

### ステップ1：Google Colabにアクセス

1. **Google Colabを開く**
   - https://colab.research.google.com/
   - Googleアカウントでログイン

2. **新しいノートブックを作成**
   - 「ファイル」→「新しいノートブック」
   - 名前を変更：`物体認識体験.ipynb`

### ステップ2：基本操作を覚える

**セル**：コードを書く場所
- `+ コード`：新しいコードセルを追加
- `+ テキスト`：説明文を追加
- `Shift + Enter`：セルを実行

### 簡単な計算をしてみよう

```python
# これはコメント（実行されない説明文）
print("こんにちは、AI！")

# 計算もできる
2 + 3
```

実行すると：
```
こんにちは、AI！
5
```

---

## 🖼️ 事前学習済みモデルで画像認識

### VGG16モデルを使った画像認識

**VGG16とは？**
- 100万枚以上の画像で学習済み
- 1000種類の物体を認識できる
- すぐに使える優秀なモデル

### ステップ1：必要なライブラリをインストール

```python
# 画像処理と機械学習のライブラリをインストール
!pip install tensorflow pillow matplotlib
```

### ステップ2：ライブラリをインポート

```python
# 必要なライブラリを読み込む
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
```

### ステップ3：モデルを読み込む

```python
# VGG16モデルを読み込む（初回は少し時間がかかります）
print("モデルを読み込んでいます...")
model = VGG16(weights='imagenet')
print("準備完了！")
```

### ステップ4：画像をアップロードして認識

```python
# 画像をアップロードする機能
from google.colab import files

print("認識したい画像をアップロードしてください")
uploaded = files.upload()

# アップロードされた画像のファイル名を取得
filename = list(uploaded.keys())[0]
print(f"画像 '{filename}' をアップロードしました")
```

### ステップ5：画像を前処理して予測

```python
# 画像を読み込んで前処理
img = image.load_img(filename, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

# 予測実行
print("AIが画像を分析中...")
predictions = model.predict(img_array)

# 結果を解釈（上位3つ）
results = decode_predictions(predictions, top=3)[0]

# 結果を表示
print("\n=== 認識結果 ===")
for i, (imagenet_id, label, score) in enumerate(results):
    print(f"{i+1}位: {label} ({score*100:.1f}%)")
```

### ステップ6：結果を視覚的に表示

```python
# 画像と結果を並べて表示
plt.figure(figsize=(10, 5))

# 左側に画像
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title('入力画像')
plt.axis('off')

# 右側に結果のグラフ
plt.subplot(1, 2, 2)
labels = [r[1] for r in results]
scores = [r[2] for r in results]
plt.barh(labels, scores)
plt.xlabel('確率')
plt.title('認識結果 Top3')
plt.xlim(0, 1)

plt.tight_layout()
plt.show()
```

---

## 🔍 特徴マップを見てみよう

AIが画像のどこに注目しているか可視化してみましょう。

```python
# モデルの中間層を取得
from tensorflow.keras.models import Model

# 最初の畳み込み層の出力を取得
layer_outputs = [layer.output for layer in model.layers[1:6]]
activation_model = Model(inputs=model.input, outputs=layer_outputs)

# 特徴マップを計算
activations = activation_model.predict(img_array)

# 最初の層の特徴マップを表示
first_layer_activation = activations[0]
plt.figure(figsize=(15, 8))

# 8個の特徴マップを表示
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')
    plt.title(f'特徴マップ {i+1}')
    plt.axis('off')

plt.suptitle('AIが見ている特徴（最初の層）')
plt.show()
```

---

## 🎮 練習問題

### 課題1：いろいろな画像で試してみよう

以下の画像で認識精度を確認：
1. 工具（ドライバー、レンチなど）
2. 金属部品
3. 自分のスマートフォンで撮った写真

### 課題2：認識結果をまとめよう

| 画像の内容 | 1位の予測 | 確率 | 正しかった？ |
|-----------|----------|------|-------------|
| 例：ドライバー | screwdriver | 92.3% | ○ |
| | | | |
| | | | |

### 課題3：考察

1. どんな画像が正しく認識されやすい？
2. 認識に失敗した画像の共通点は？
3. 工場で使うにはどんな改善が必要？

---

## 💡 発展課題：人物認識を追加してみよう

```python
# 1. 基本ライブラリの確認
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO

# 2. VGG16関連のインポート  
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions

print("✅ すべてのライブラリが正常にインポートされました")
```


```python
# 関数定義
def yolo_person_detection(image_path):
    """
    YOLOv8を使った人物検出
    """
    print("=== YOLO人物検出 ===")
    
    # YOLOv8モデルをロード（初回は自動ダウンロード）
    model = YOLO('yolov8n.pt')  # nano版（軽量）
    
    # 画像を読み込み
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # 推論実行
    results = model(image_path)
    
    # 人物（クラス0）のみを抽出
    person_detections = []
    for r in results:
        boxes = r.boxes
        if boxes is not None:
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                
                # クラス0が人物
                if class_id == 0 and confidence > 0.5:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    person_detections.append({
                        'bbox': (int(x1), int(y1), int(x2), int(y2)),
                        'confidence': confidence
                    })
    
    # 結果を描画
    result_image = image_rgb.copy()
    for detection in person_detections:
        x1, y1, x2, y2 = detection['bbox']
        confidence = detection['confidence']
        
        # バウンディングボックスを描画
        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(result_image, f'Person: {confidence:.2f}', 
                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    print(f"検出された人物数: {len(person_detections)}")
    for i, detection in enumerate(person_detections):
        print(f"人物{i+1}: 信頼度 {detection['confidence']:.3f}")
    
    return result_image, person_detections

def opencv_person_detection(image_path):
    """
    OpenCVのHOG + SVMを使った人物検出
    """
    print("\n=== OpenCV人物検出 ===")
    
    # 画像を読み込み
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # HOG記述子を初期化
    hog = cv2.HOGDescriptor()
    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
    
    # 人物検出を実行
    boxes, weights = hog.detectMultiScale(
        image_rgb,
        winStride=(8, 8),
        padding=(32, 32),
        scale=1.05,
        useMeanshiftGrouping=False
    )
    
    # 結果を描画
    result_image = image_rgb.copy()
    person_detections = []
    
    for i, (x, y, w, h) in enumerate(boxes):
        confidence = weights[i][0] if len(weights) > i else 0.5
        
        # バウンディングボックスを描画
        cv2.rectangle(result_image, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(result_image, f'Person: {confidence:.2f}', 
                   (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        person_detections.append({
            'bbox': (x, y, x + w, y + h),
            'confidence': confidence
        })
    
    print(f"検出された人物数: {len(person_detections)}")
    for i, detection in enumerate(person_detections):
        print(f"人物{i+1}: 信頼度 {detection['confidence']:.3f}")
    
    return result_image, person_detections

def opencv_person_detection(image_path):
    """
    OpenCVのHOG + SVMを使った人物検出（修正版）
    """
    print("\n=== OpenCV人物検出 ===")
    
    # 画像を読み込み
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # HOG記述子を初期化
    hog = cv2.HOGDescriptor()
    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
    
    # 人物検出を実行
    boxes, weights = hog.detectMultiScale(
        image_rgb,
        winStride=(8, 8),
        padding=(32, 32),
        scale=1.05,
        useMeanshiftGrouping=False
    )
    
    # 結果を描画
    result_image = image_rgb.copy()
    person_detections = []
    
    for i, (x, y, w, h) in enumerate(boxes):
        # weightsの形状を確認して適切に処理
        if len(weights) > 0:
            if weights.ndim > 1 and weights.shape[1] > 0:
                confidence = weights[i][0] if i < len(weights) else 0.5
            else:
                confidence = weights[i] if i < len(weights) else 0.5
        else:
            confidence = 0.5
        
        # バウンディングボックスを描画
        cv2.rectangle(result_image, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(result_image, f'Person: {confidence:.2f}', 
                   (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        person_detections.append({
            'bbox': (x, y, x + w, y + h),
            'confidence': float(confidence)
        })
    
    print(f"検出された人物数: {len(person_detections)}")
    for i, detection in enumerate(person_detections):
        print(f"人物{i+1}: 信頼度 {detection['confidence']:.3f}")
    
    return result_image, person_detections

# 修正版を定義しました
print("✅ OpenCV人物検出関数を修正しました")

def detect_all_methods(image_path):
    """
    すべての検出方法を実行して比較
    """
    print("画像の人物・顔検出を開始...")
    print(f"対象画像: {image_path}")
    
    # 各手法で検出実行
    yolo_result, yolo_detections = yolo_person_detection(image_path)
    opencv_result, opencv_detections = opencv_person_detection(image_path)
    face_result, face_detections = opencv_face_detection(image_path)
    
    # 結果を可視化
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.imshow(yolo_result)
    plt.title(f'YOLO検出 ({len(yolo_detections)}人)')
    plt.axis('off')
    
    plt.subplot(1, 3, 2)
    plt.imshow(opencv_result)
    plt.title(f'OpenCV HOG検出 ({len(opencv_detections)}人)')
    plt.axis('off')
    
    plt.subplot(1, 3, 3)
    plt.imshow(face_result)
    plt.title(f'OpenCV顔検出 ({len(face_detections)}顔)')
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # 総合結果
    print("\n=== 総合結果 ===")
    print(f"YOLO: {len(yolo_detections)}人検出")
    print(f"OpenCV HOG: {len(opencv_detections)}人検出") 
    print(f"OpenCV 顔検出: {len(face_detections)}顔検出")
    
    return {
        'yolo': yolo_detections,
        'opencv_hog': opencv_detections,
        'face': face_detections
    }

# 関数定義が完了しました
print("✅ 人物検出関数が定義されました")
```


```python
# VGG16モデルをロード
model = VGG16(weights='imagenet')

# Google Colabでの画像アップロード機能と統合
from google.colab import files

print("認識したい画像をアップロードしてください")
uploaded = files.upload()

# アップロードされた画像のファイル名を取得
filename = list(uploaded.keys())[0]
print(f"画像 '{filename}' をアップロードしました")

# VGG16による分類（既存のコード）
print("\n" + "="*50)
print("VGG16による画像分類")
print("="*50)

# 画像を読み込んで前処理
img = image.load_img(filename, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

# 予測実行
print("AIが画像を分析中...")
predictions = model.predict(img_array)

# 結果を解釈（上位3つ）
results = decode_predictions(predictions, top=3)[0]

# 結果を表示
print("\n=== VGG16認識結果 ===")
for i, (imagenet_id, label, score) in enumerate(results):
    print(f"{i+1}位: {label} ({score*100:.1f}%)")

# 人物・顔検出の実行
print("\n" + "="*50)
print("人物・顔検出の開始")
print("="*50)

# すべての検出手法を実行
detection_results = detect_all_methods(filename)
```

---

## 🤔 よくある質問

**Q: なぜ224×224のサイズにするの？**
A: VGG16モデルがその大きさで学習されたから。モデルごとに決まったサイズがあります。

**Q: 日本語で結果が出ない**
A: ImageNetは英語のデータセット。でも1000種類も認識できてすごい！

**Q: 工場の部品は認識できる？**
A: 一般的な物体は認識できるけど、特殊な部品は難しい。だから自分でモデルを作る必要があります。

---

## 📊 今日のまとめ

### 学んだこと
- [ ] 機械学習の基本的な仕組み
- [ ] Google Colabの使い方
- [ ] 画像認識の体験
- [ ] AIが見ている特徴の可視化

### 重要なポイント
1. **データが大事**：たくさんの良質なデータで学習
2. **前処理が必要**：画像のサイズや形式を統一
3. **確率で判定**：100%ではなく、可能性を示す

### 次回予告
次回は「Teachable Machine」を使って、自分だけのAIモデルを作ります！
工場で使える「良品/不良品」判定AIを作ってみましょう。

---

## 🏠 宿題

1. **実験レポート**
   - 5種類以上の画像で実験
   - 結果をまとめて提出

2. **調査課題**
   - 「Teachable Machine」について調べる
   - どんなことができそうか3つ考える

3. **準備**
   - 次回使う画像を考える（良品/不良品のサンプル）

**提出方法**: 
- Google Colabのリンクを共有
- GitHubのnotebooksフォルダにアップロード